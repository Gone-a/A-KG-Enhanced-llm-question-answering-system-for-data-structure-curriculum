#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä»ç¼“å­˜æ•°æ®ç”Ÿæˆæœ€ç»ˆæ–‡ä»¶çš„è„šæœ¬
"""

import json
import os
import re
from collections import defaultdict

def load_cache_data():
    """åŠ è½½ç¼“å­˜æ•°æ®"""
    cache_file = "/root/KG_inde/generate_data/data_backups/api_cache.json"
    
    if not os.path.exists(cache_file):
        print(f"âŒ ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_file}")
        return {}
    
    with open(cache_file, 'r', encoding='utf-8') as f:
        cache_data = json.load(f)
    
    print(f"ğŸ“¦ åŠ è½½ç¼“å­˜æ•°æ®: {len(cache_data)} æ¡è®°å½•")
    return cache_data

def parse_sentences_from_cache(cache_data):
    """ä»ç¼“å­˜æ•°æ®ä¸­è§£æå¥å­"""
    all_sentences = []
    relation_counts = defaultdict(int)
    
    # å…³ç³»ç±»å‹æ˜ å°„
    relation_types = {
        "hasComplexity": "ç®—æ³•çš„å¤æ‚åº¦å±æ€§",
        "uses": "ç®—æ³•ä½¿ç”¨çš„æ•°æ®ç»“æ„", 
        "variantOf": "æ•°æ®ç»“æ„çš„å˜ä½“å…³ç³»",
        "appliesTo": "æ•°æ®ç»“æ„çš„åº”ç”¨åœºæ™¯",
        "provides": "æ•°æ®ç»“æ„æä¾›çš„æ“ä½œ",
        "implementedAs": "æ•°æ®ç»“æ„çš„å®ç°æ–¹å¼",
        "usedIn": "æ“ä½œçš„åº”ç”¨åœºæ™¯"
    }
    
    for cache_key, response_text in cache_data.items():
        # æŒ‰è¡Œåˆ†å‰²å“åº”æ–‡æœ¬
        lines = response_text.strip().split('\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # å°è¯•è¯†åˆ«å…³ç³»ç±»å‹å’Œå®ä½“
            sentence_data = parse_sentence(line, relation_types)
            if sentence_data:
                all_sentences.append(sentence_data)
                relation_counts[sentence_data['relation']] += 1
    
    print(f"âœ… è§£æå®Œæˆï¼Œå…± {len(all_sentences)} æ¡å¥å­")
    for relation, count in relation_counts.items():
        print(f"  - {relation}: {count} æ¡")
    
    return all_sentences, relation_counts

def parse_sentence(sentence, relation_types):
    """è§£æå•ä¸ªå¥å­ï¼Œè¯†åˆ«å…³ç³»å’Œå®ä½“"""
    
    # å®šä¹‰å…³ç³»è¯†åˆ«æ¨¡å¼
    patterns = {
        "hasComplexity": [
            r"(.+?)ç®—æ³•.*?å¤æ‚åº¦.*?([OÎ˜Î©]\([^)]+\)|æ—¶é—´å¤æ‚åº¦|ç©ºé—´å¤æ‚åº¦|[OÎ˜Î©]è®°å·|å¹³å‡æŸ¥æ‰¾é•¿åº¦|åŠ¿èƒ½æ–¹æ³•|æ‘Šè¿˜åˆ†æ)",
            r"(.+?)çš„.*?å¤æ‚åº¦.*?([OÎ˜Î©]\([^)]+\)|æ—¶é—´å¤æ‚åº¦|ç©ºé—´å¤æ‚åº¦|[OÎ˜Î©]è®°å·|å¹³å‡æŸ¥æ‰¾é•¿åº¦|åŠ¿èƒ½æ–¹æ³•|æ‘Šè¿˜åˆ†æ)",
        ],
        "uses": [
            r"(.+?)ç®—æ³•.*?ä½¿ç”¨.*?([^ã€‚ï¼Œ,\n]+?)(?:æ¥å®ç°|æ•°æ®ç»“æ„|è¿›è¡Œ|ä½œä¸º)",
            r"(.+?).*?ä¾èµ–.*?([^ã€‚ï¼Œ,\n]+?)æ•°æ®ç»“æ„",
            r"(.+?).*?é‡‡ç”¨.*?([^ã€‚ï¼Œ,\n]+?)(?:ä½œä¸º|è¿›è¡Œ)",
        ],
        "variantOf": [
            r"([^ã€‚ï¼Œ,\n]+?)æ˜¯([^ã€‚ï¼Œ,\n]+?)çš„.*?å˜ä½“",
            r"([^ã€‚ï¼Œ,\n]+?)å±äº([^ã€‚ï¼Œ,\n]+?)çš„.*?å½¢å¼",
            r"([^ã€‚ï¼Œ,\n]+?).*?åŸºäº([^ã€‚ï¼Œ,\n]+?)æ”¹è¿›",
        ],
        "appliesTo": [
            r"([^ã€‚ï¼Œ,\n]+?).*?åº”ç”¨äº([^ã€‚ï¼Œ,\n]+?)åœºæ™¯",
            r"([^ã€‚ï¼Œ,\n]+?).*?é€‚åˆ.*?([^ã€‚ï¼Œ,\n]+?)(?:åœºæ™¯|é—®é¢˜)",
            r"([^ã€‚ï¼Œ,\n]+?).*?ç”¨äº([^ã€‚ï¼Œ,\n]+?)(?:åœºæ™¯|é—®é¢˜|ä¸­)",
        ],
        "provides": [
            r"([^ã€‚ï¼Œ,\n]+?)æä¾›.*?([^ã€‚ï¼Œ,\n]+?)(?:åŠŸèƒ½|æ“ä½œ)",
            r"([^ã€‚ï¼Œ,\n]+?)æ”¯æŒ([^ã€‚ï¼Œ,\n]+?)æ“ä½œ",
            r"([^ã€‚ï¼Œ,\n]+?).*?å®ç°([^ã€‚ï¼Œ,\n]+?)(?:åŠŸèƒ½|æ“ä½œ)",
        ],
        "implementedAs": [
            r"([^ã€‚ï¼Œ,\n]+?).*?é€šè¿‡([^ã€‚ï¼Œ,\n]+?)æ¥å®ç°",
            r"([^ã€‚ï¼Œ,\n]+?).*?é‡‡ç”¨.*?([^ã€‚ï¼Œ,\n]+?)æ–¹æ³•",
            r"([^ã€‚ï¼Œ,\n]+?).*?åŸºäº([^ã€‚ï¼Œ,\n]+?)è¿›è¡Œå®ç°",
        ],
        "usedIn": [
            r"([^ã€‚ï¼Œ,\n]+?)æ“ä½œ.*?ç”¨äº([^ã€‚ï¼Œ,\n]+?)(?:åœºæ™¯|ä¸­)",
            r"([^ã€‚ï¼Œ,\n]+?).*?å¸¸ç”¨äº([^ã€‚ï¼Œ,\n]+?)(?:åœºæ™¯|é—®é¢˜)",
            r"([^ã€‚ï¼Œ,\n]+?).*?åº”ç”¨.*?([^ã€‚ï¼Œ,\n]+?)åœºæ™¯",
        ]
    }
    
    # å°è¯•åŒ¹é…æ¯ç§å…³ç³»ç±»å‹
    for relation, pattern_list in patterns.items():
        for pattern in pattern_list:
            match = re.search(pattern, sentence)
            if match:
                head = match.group(1).strip()
                tail = match.group(2).strip()
                
                # æ¸…ç†å®ä½“åç§°
                head = clean_entity(head)
                tail = clean_entity(tail)
                
                if head and tail and head != tail:
                    return {
                        'sentence': sentence,
                        'relation': relation,
                        'head': head,
                        'tail': tail
                    }
    
    return None

def clean_entity(entity):
    """æ¸…ç†å®ä½“åç§°"""
    # ç§»é™¤å¸¸è§çš„ä¿®é¥°è¯
    entity = re.sub(r'^(åœ¨|å½“|ä½¿ç”¨|é€šè¿‡|ä¸ºäº†|å®ç°|åˆ†æ|æ„å»º)', '', entity)
    entity = re.sub(r'(ç®—æ³•|æ•°æ®ç»“æ„|é—®é¢˜|åœºæ™¯|æ“ä½œ|æ–¹æ³•|è¿‡ç¨‹|æ—¶|ä¸­)$', '', entity)
    entity = entity.strip('ï¼Œ,ã€‚ã€')
    return entity.strip()

def save_data(sentences, output_dir="/root/KG_inde/generate_data/data_backups"):
    """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜ä¸ºJSONæ ¼å¼
    json_file = os.path.join(output_dir, "knowledge_graph_sentences_from_cache.json")
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(sentences, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜ä¸ºæ–‡æœ¬æ ¼å¼
    txt_file = os.path.join(output_dir, "knowledge_graph_sentences_from_cache.txt")
    with open(txt_file, 'w', encoding='utf-8') as f:
        for item in sentences:
            f.write(f"{item['sentence']}\t{item['relation']}\t{item['head']}\t{item['tail']}\n")
    
    print(f"âœ… æ•°æ®å·²ä¿å­˜:")
    print(f"  - JSONæ ¼å¼: {json_file}")
    print(f"  - æ–‡æœ¬æ ¼å¼: {txt_file}")
    
    return json_file, txt_file

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ä»ç¼“å­˜ç”ŸæˆçŸ¥è¯†å›¾è°±æ•°æ®...")
    
    # åŠ è½½ç¼“å­˜æ•°æ®
    cache_data = load_cache_data()
    if not cache_data:
        return
    
    # è§£æå¥å­
    sentences, relation_counts = parse_sentences_from_cache(cache_data)
    
    if not sentences:
        print("âŒ æ²¡æœ‰è§£æåˆ°æœ‰æ•ˆçš„å¥å­æ•°æ®")
        return
    
    # ä¿å­˜æ•°æ®
    json_file, txt_file = save_data(sentences)
    
    print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"æ€»å¥å­æ•°: {len(sentences)}")
    for relation, count in relation_counts.items():
        percentage = (count / len(sentences)) * 100 if sentences else 0
        print(f"  - {relation}: {count} æ¡ ({percentage:.1f}%)")
    
    print(f"\nâœ… ä»ç¼“å­˜ç”Ÿæˆæ•°æ®å®Œæˆ!")

if __name__ == "__main__":
    main()