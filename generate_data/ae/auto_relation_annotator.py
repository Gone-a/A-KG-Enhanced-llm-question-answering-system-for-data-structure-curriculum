#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨å…³ç³»æ ‡æ³¨å·¥å…·
åŸºäºç«å±±å¼•æ“å¤§æ¨¡å‹APIï¼Œè‡ªåŠ¨ä¸ºå®ä½“å¯¹æ ‡æ³¨å…³ç³»
ç»“åˆae.pyçš„APIè°ƒç”¨æ¨¡å¼å’Œrelation_annotation_cli.pyçš„å…³ç³»è§„åˆ™
"""

import os
import json
import csv
import time
import re
import hashlib
from typing import List, Dict, Tuple, Optional
from datetime import datetime
from dotenv import load_dotenv
from volcenginesdkarkruntime import Ark
import concurrent.futures
from threading import Lock

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class AutoRelationAnnotator:
    """è‡ªåŠ¨å…³ç³»æ ‡æ³¨å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–é…ç½®"""
        # APIé…ç½®
        self.api_key = os.getenv("ARK_API_KEY")
        if not self.api_key:
            raise EnvironmentError("è¯·è®¾ç½®ç«å±±å¼•æ“APIå¯†é’¥")
        
        self.api_key = self.api_key.strip()
        self.client = Ark(api_key=self.api_key)
        self.model_id = "doubao-seed-1-6-thinking-250715"
        
        # APIè°ƒç”¨ä¼˜åŒ–å‚æ•°
        self.api_timeout = 120
        self.retry_attempts = 2
        self.retry_delay = 2
        self.max_workers = 40
        
        # ç¼“å­˜é…ç½®
        self.cache = {}
        self.cache_lock = Lock()
        self.cache_file = "cache/relation_api_cache.json"
        self._last_cache_size = 0
        
        # è¿›åº¦è·Ÿè¸ªé…ç½®
        self.progress_file = "annotation_progress.json"
        self.processed_indices = set()  # å·²å¤„ç†çš„æ•°æ®ç´¢å¼•
        
        # æ–‡ä»¶è·¯å¾„
        self.vocab_file = "vocab_dict.csv"
        self.relation_file = "relation.csv"
        self.input_file = "filtered.json"
        self.output_file = "predict_with_relations.json"
        self.csv_output_file = "DeepKE/example/re/standard/data/example.csv"
        
        # æ•°æ®å­˜å‚¨
        self.entity_types = {}  # å®ä½“åˆ°ç±»å‹çš„æ˜ å°„
        self.relations = {}     # å…³ç³»è§„åˆ™
        self.data = []          # è¾“å…¥æ•°æ®
        self.annotated_data = [] # æ ‡æ³¨åçš„æ•°æ®
        
        # å…³ç³»è§„åˆ™è¯¦ç»†è¯´æ˜
        self.relation_rules_details = {
            'hasComplexity': {
                'definition': 'ç®—æ³•çš„å¤æ‚åº¦å±æ€§ï¼ˆç»Ÿä¸€å¤„ç†æ—¶é—´/ç©ºé—´/æœ€å/å¹³å‡ç­‰ï¼‰',
                'type_constraint': 'Algorithm â†’ Complexity',
                'annotation_condition': 'æ–‡ä»¶æ˜ç¡®æåˆ°"ç®—æ³•çš„å¤æ‚åº¦"ï¼ˆå¦‚"æ—¶é—´å¤æ‚åº¦ä¸ºO(n log n)"ã€"å¹³å‡æƒ…å†µå¤æ‚åº¦ä¸ºO(n)"ï¼‰æˆ–åŸºäºç®—æ³•ç‰¹æ€§å¯åˆç†æ¨æ–­å…¶å¤æ‚åº¦',
                'key_points': 'æ ‡æ³¨å¤æ‚åº¦å€¼ï¼ˆå¦‚O(n log n)ï¼‰ï¼Œå¯åŸºäºç®—æ³•ç±»å‹å’Œä¸“ä¸šçŸ¥è¯†æ¨æ–­å…¸å‹å¤æ‚åº¦',
                'knowledge_inference': 'å¯åŸºäºç®—æ³•ç±»å‹æ¨æ–­ï¼šå¦‚å¿«é€Ÿæ’åºé€šå¸¸ä¸ºO(n log n)ï¼Œçº¿æ€§æœç´¢ä¸ºO(n)ç­‰'
            },
            'uses': {
                'definition': 'ç®—æ³•ä¾èµ–æˆ–ä½¿ç”¨çš„æ•°æ®ç»“æ„',
                'type_constraint': 'Algorithm â†’ DataStructure',
                'annotation_condition': 'æ–‡ä»¶æ˜ç¡®æåˆ°"ç®—æ³•ä½¿ç”¨æ•°æ®ç»“æ„"ï¼ˆå¦‚"æœ€çŸ­è·¯å¾„ç®—æ³•ä½¿ç”¨å›¾"ã€"DFSä½¿ç”¨æ ˆ"ï¼‰æˆ–åŸºäºç®—æ³•åŸç†å¯æ¨æ–­å…¶ä½¿ç”¨çš„æ•°æ®ç»“æ„',
                'key_points': 'æ ‡æ³¨ç®—æ³•ç›´æ¥ä½¿ç”¨çš„æ•°æ®ç»“æ„ï¼Œå¯åŸºäºç®—æ³•å®ç°åŸç†è¿›è¡Œåˆç†æ¨æ–­',
                'knowledge_inference': 'å¯åŸºäºç®—æ³•ç‰¹æ€§æ¨æ–­ï¼šå¦‚å›¾ç®—æ³•ä½¿ç”¨å›¾ç»“æ„ï¼Œæ’åºç®—æ³•å¯èƒ½ä½¿ç”¨æ•°ç»„ï¼Œæœç´¢ç®—æ³•å¯èƒ½ä½¿ç”¨æ ‘ç­‰'
            },
            'variantOf': {
                'definition': 'æ•°æ®ç»“æ„çš„å˜ä½“/æ´¾ç”Ÿå…³ç³»',
                'type_constraint': 'DataStructure â†’ DataStructure',
                'annotation_condition': 'æ–‡ä»¶æ˜ç¡®æåˆ°"æ˜¯...çš„å˜ä½“"ï¼ˆå¦‚"B+æ ‘æ˜¯Bæ ‘çš„å˜ä½“"ã€"å¹³è¡¡äºŒå‰æ ‘æ˜¯äºŒå‰æ ‘çš„å˜ä½“"ï¼‰æˆ–åŸºäºæ•°æ®ç»“æ„ç†è®ºå¯æ¨æ–­çš„å˜ä½“å…³ç³»',
                'key_points': 'æ ‡æ³¨æ˜ç¡®çš„å˜ä½“å…³ç³»ï¼Œå¯åŸºäºæ•°æ®ç»“æ„åˆ†ç±»å­¦è¿›è¡Œæ¨ç†',
                'knowledge_inference': 'å¯åŸºäºç»“æ„ç‰¹æ€§æ¨æ–­ï¼šå¦‚çº¢é»‘æ ‘æ˜¯å¹³è¡¡äºŒå‰æœç´¢æ ‘çš„å˜ä½“ï¼Œå †æ˜¯å®Œå…¨äºŒå‰æ ‘çš„å˜ä½“ç­‰'
            },
            'appliesTo': {
                'definition': 'æ•°æ®ç»“æ„çš„å…¸å‹åº”ç”¨åœºæ™¯',
                'type_constraint': 'DataStructure â†’ ApplicationScenario',
                'annotation_condition': 'æ–‡ä»¶æ˜ç¡®æåˆ°"æ•°æ®ç»“æ„ç”¨äºåœºæ™¯"ï¼ˆå¦‚"æ ˆç”¨äºè¡¨è¾¾å¼æ±‚å€¼"ã€"é˜Ÿåˆ—å¸¸ç”¨äºä»»åŠ¡è°ƒåº¦"ï¼‰æˆ–åŸºäºæ•°æ®ç»“æ„ç‰¹æ€§å¯æ¨æ–­çš„å…¸å‹åº”ç”¨',
                'key_points': 'æ ‡æ³¨å…¸å‹åº”ç”¨åœºæ™¯ï¼Œå¯åŸºäºæ•°æ®ç»“æ„ç‰¹æ€§å’Œè®¡ç®—æœºç§‘å­¦å¸¸è¯†è¿›è¡Œæ¨ç†',
                'knowledge_inference': 'å¯åŸºäºç»“æ„ç‰¹æ€§æ¨æ–­ï¼šå¦‚æ ˆé€‚ç”¨äºåè¿›å…ˆå‡ºåœºæ™¯ï¼Œé˜Ÿåˆ—é€‚ç”¨äºå…ˆè¿›å…ˆå‡ºåœºæ™¯ï¼Œå“ˆå¸Œè¡¨é€‚ç”¨äºå¿«é€ŸæŸ¥æ‰¾ç­‰'
            },
            'provides': {
                'definition': 'æ•°æ®ç»“æ„æ”¯æŒçš„æ“ä½œ',
                'type_constraint': 'DataStructure â†’ Operation',
                'annotation_condition': 'æ–‡ä»¶æ˜ç¡®æåˆ°"æ•°æ®ç»“æ„æä¾›æ“ä½œ"ï¼ˆå¦‚"æ ˆæä¾›å…¥æ ˆå’Œå‡ºæ ˆæ“ä½œ"ï¼‰æˆ–åŸºäºæ•°æ®ç»“æ„å®šä¹‰å¯æ¨æ–­çš„åŸºæœ¬æ“ä½œ',
                'key_points': 'æ ‡æ³¨æ•°æ®ç»“æ„çš„åŸºæœ¬æ“ä½œï¼Œå¯åŸºäºæ•°æ®ç»“æ„ç†è®ºæ¨æ–­æ ‡å‡†æ“ä½œé›†',
                'knowledge_inference': 'å¯åŸºäºç»“æ„å®šä¹‰æ¨æ–­ï¼šå¦‚æ ˆæä¾›push/popæ“ä½œï¼Œé˜Ÿåˆ—æä¾›enqueue/dequeueæ“ä½œï¼Œæ ‘æä¾›éå†æ“ä½œç­‰'
            },
            'implementedAs': {
                'definition': 'æ•°æ®ç»“æ„çš„å®ç°æ–¹å¼',
                'type_constraint': 'DataStructure â†’ Algorithm',
                'annotation_condition': 'æ–‡ä»¶æ˜ç¡®æåˆ°"æ•°æ®ç»“æ„ç”¨æŸç§æ–¹å¼å®ç°"ï¼ˆå¦‚"é˜Ÿåˆ—å¯ç”¨æ•°ç»„å®ç°"ã€"é“¾è¡¨å®ç°é˜Ÿåˆ—"ï¼‰æˆ–åŸºäºå®ç°ç†è®ºå¯æ¨æ–­çš„å®ç°æ–¹å¼',
                'key_points': 'æ ‡æ³¨å®ç°æ–¹å¼ï¼Œå¯åŸºäºè®¡ç®—æœºç§‘å­¦ç†è®ºå’Œå®è·µç»éªŒè¿›è¡Œåˆç†æ¨æ–­',
                'knowledge_inference': 'å¯åŸºäºå®ç°åŸç†æ¨æ–­ï¼šå¦‚åŠ¨æ€æ•°ç»„å¯ç”¨é™æ€æ•°ç»„å®ç°ï¼Œå›¾å¯ç”¨é‚»æ¥è¡¨æˆ–é‚»æ¥çŸ©é˜µå®ç°ç­‰'
            },
            'usedIn': {
                'definition': 'æ“ä½œçš„å…¸å‹åº”ç”¨åœºæ™¯',
                'type_constraint': 'Operation â†’ ApplicationScenario',
                'annotation_condition': 'æ–‡ä»¶æ˜ç¡®æåˆ°"æ“ä½œç”¨äºåœºæ™¯"ï¼ˆå¦‚"å…¥æ ˆç”¨äºæ‹¬å·åŒ¹é…"ã€"å‡ºæ ˆç”¨äºè¡¨è¾¾å¼æ±‚å€¼"ï¼‰æˆ–åŸºäºæ“ä½œç‰¹æ€§å¯æ¨æ–­çš„åº”ç”¨åœºæ™¯',
                'key_points': 'æ ‡æ³¨æ“ä½œçš„å…¸å‹åº”ç”¨åœºæ™¯ï¼Œå¯åŸºäºæ“ä½œè¯­ä¹‰å’Œè®¡ç®—æœºç§‘å­¦åº”ç”¨è¿›è¡Œæ¨ç†',
                'knowledge_inference': 'å¯åŸºäºæ“ä½œç‰¹æ€§æ¨æ–­ï¼šå¦‚é€’å½’æ“ä½œç”¨äºåˆ†æ²»ç®—æ³•ï¼Œæ¯”è¾ƒæ“ä½œç”¨äºæ’åºç®—æ³•ç­‰'
            }
        }
        
        # åˆå§‹åŒ–
        self.load_cache()
        self.load_progress()  # åŠ è½½è¿›åº¦ä¿¡æ¯
        self.load_entity_types()
        self.load_relations()
        self.load_data()
        
        # åˆå§‹åŒ–CSVæ–‡ä»¶
        self.init_csv_file()
        
        print(f"âœ… åˆå§‹åŒ–å®Œæˆ")
        print(f"   - å®ä½“ç±»å‹: {len(self.entity_types)} ä¸ª")
        print(f"   - å…³ç³»è§„åˆ™: {len(self.relations)} ä¸ª")
        print(f"   - è¾“å…¥æ•°æ®: {len(self.data)} æ¡")
        print(f"   - CSVè¾“å‡ºæ–‡ä»¶: {self.csv_output_file}")
        
        # æ˜¾ç¤ºæ–­ç‚¹ç»­ç”¨çŠ¶æ€
        if len(self.processed_indices) > 0:
            remaining = len(self.data) - len(self.processed_indices)
            print(f"   - æ–­ç‚¹ç»­ç”¨: å·²å¤„ç† {len(self.processed_indices)} æ¡ï¼Œå‰©ä½™ {remaining} æ¡")
        else:
            print(f"   - å¤„ç†æ¨¡å¼: ä»å¤´å¼€å§‹å¤„ç†")
    
    def load_cache(self):
        """åŠ è½½ç¼“å­˜æ–‡ä»¶"""
        try:
            os.makedirs(os.path.dirname(self.cache_file), exist_ok=True)
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    self.cache = json.load(f)
                print(f"âœ… åŠ è½½ç¼“å­˜: {len(self.cache)} æ¡è®°å½•")
                self._last_cache_size = len(self.cache)
            else:
                self.cache = {}
                self._last_cache_size = 0
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
            self.cache = {}
            self._last_cache_size = 0
    
    def load_progress(self):
        """åŠ è½½è¿›åº¦æ–‡ä»¶"""
        try:
            if os.path.exists(self.progress_file):
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    progress_data = json.load(f)
                    self.processed_indices = set(progress_data.get('processed_indices', []))
                print(f"âœ… åŠ è½½è¿›åº¦: å·²å¤„ç† {len(self.processed_indices)} æ¡æ•°æ®")
            else:
                self.processed_indices = set()
                print(f"ğŸ“ æœªæ‰¾åˆ°è¿›åº¦æ–‡ä»¶ï¼Œä»å¤´å¼€å§‹å¤„ç†")
        except Exception as e:
            print(f"âš ï¸ è¿›åº¦åŠ è½½å¤±è´¥: {e}")
            self.processed_indices = set()
    
    def save_progress(self):
        """ä¿å­˜è¿›åº¦åˆ°æ–‡ä»¶"""
        try:
            progress_data = {
                'processed_indices': list(self.processed_indices),
                'last_updated': datetime.now().isoformat(),
                'total_processed': len(self.processed_indices)
            }
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ è¿›åº¦ä¿å­˜å¤±è´¥: {e}")
    
    def init_csv_file(self):
        """åˆå§‹åŒ–CSVè¾“å‡ºæ–‡ä»¶"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(self.csv_output_file), exist_ok=True)
            
            # å¦‚æœæœ‰è¿›åº¦è®°å½•ä¸”CSVæ–‡ä»¶å­˜åœ¨ï¼Œåˆ™ä¸é‡æ–°åˆå§‹åŒ–
            if len(self.processed_indices) > 0 and os.path.exists(self.csv_output_file):
                print(f"âœ… æ£€æµ‹åˆ°æ–­ç‚¹ç»­ç”¨ï¼Œä¿ç•™ç°æœ‰CSVæ–‡ä»¶: {self.csv_output_file}")
                return True
            
            # åˆ›å»ºCSVæ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´
            with open(self.csv_output_file, 'w', encoding='utf-8', newline='') as f:
                writer = csv.writer(f)
                writer.writerow(['sentence', 'relation', 'head', 'head_offset', 'tail', 'tail_offset'])
            
            print(f"âœ… CSVæ–‡ä»¶åˆå§‹åŒ–å®Œæˆ: {self.csv_output_file}")
            return True
        except Exception as e:
            print(f"âŒ CSVæ–‡ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def save_cache(self):
        """ä¿å­˜ç¼“å­˜åˆ°æ–‡ä»¶"""
        try:
            with self.cache_lock:
                if len(self.cache) == self._last_cache_size:
                    return
                
                with open(self.cache_file, 'w', encoding='utf-8') as f:
                    json.dump(self.cache, f, ensure_ascii=False, indent=2)
                
                self._last_cache_size = len(self.cache)
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    def load_entity_types(self):
        """ä»vocab_dict.csvåŠ è½½å®ä½“ç±»å‹æ˜ å°„"""
        try:
            with open(self.vocab_file, 'r', encoding='utf-8') as f:
                reader = csv.reader(f)
                for row in reader:
                    if len(row) >= 2:
                        entity, entity_type = row[0].strip(), row[1].strip()
                        self.entity_types[entity] = entity_type
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½å®ä½“ç±»å‹æ˜ å°„å¤±è´¥: {e}")
            return False
    
    def load_relations(self):
        """åŠ è½½å…³ç³»è§„åˆ™"""
        try:
            with open(self.relation_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    if (row.get('relation') and 
                        row.get('head_type') and 
                        row.get('tail_type') and
                        row.get('index')):
                        self.relations[row['relation']] = {
                            'head_type': row['head_type'],
                            'tail_type': row['tail_type'],
                            'index': int(row['index'])
                        }
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½å…³ç³»è§„åˆ™å¤±è´¥: {e}")
            return False
    
    def load_data(self):
        """åŠ è½½é¢„æµ‹æ•°æ®"""
        try:
            with open(self.input_file, 'r', encoding='utf-8') as f:
                self.data = json.load(f)
            
            # å»é‡å¤„ç†
            seen = set()
            unique_data = []
            for item in self.data:
                key = (item['sentence'], item['head'], item['tail'], 
                      item['head_offset'], item['tail_offset'])
                if key not in seen:
                    seen.add(key)
                    unique_data.append(item)
            
            self.data = unique_data
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return False
    
    def get_entity_type(self, entity):
        """è·å–å®ä½“çš„ç±»å‹"""
        return self.entity_types.get(entity, "Unknown")
    
    def get_available_relations(self, head_type, tail_type):
        """è·å–å¯ç”¨çš„å…³ç³»åˆ—è¡¨"""
        available = []
        for relation, rule in self.relations.items():
            if rule['head_type'] == head_type and rule['tail_type'] == tail_type:
                available.append(relation)
        return available
    
    def generate_prompt(self, item):
        """ä¸ºå®ä½“å¯¹ç”Ÿæˆå¤§æ¨¡å‹æç¤ºè¯"""
        head_type = self.get_entity_type(item['head'])
        tail_type = self.get_entity_type(item['tail'])
        
        # è·å–å¯ç”¨å…³ç³»
        available_relations = self.get_available_relations(head_type, tail_type)
        
        if not available_relations:
            return None
        
        # æ„å»ºå…³ç³»è§„åˆ™è¯´æ˜
        relation_descriptions = []
        for relation in available_relations:
            if relation in self.relation_rules_details:
                rule = self.relation_rules_details[relation]
                relation_descriptions.append(
                    f"- {relation}: {rule['definition']}\n"
                    f"  ç±»å‹çº¦æŸ: {rule['type_constraint']}\n"
                    f"  æ ‡æ³¨æ¡ä»¶: {rule['annotation_condition']}\n"
                    f"  å…³é”®ç‚¹: {rule['key_points']}\n"
                    f"  çŸ¥è¯†æ¨ç†: {rule['knowledge_inference']}"
                )
        
        relations_str = "\n\n".join(relation_descriptions)
        
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªè®¡ç®—æœºç§‘å­¦ä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹å¥å­ä¸­å®ä½“å¯¹çš„å…³ç³»ã€‚

å¥å­: "{item['sentence']}"
å¤´å®ä½“: {item['head']} (ç±»å‹: {head_type})
å°¾å®ä½“: {item['tail']} (ç±»å‹: {tail_type})

å¯ç”¨å…³ç³»è§„åˆ™:
{relations_str}

ä»»åŠ¡è¦æ±‚:
1. é¦–å…ˆä»”ç»†åˆ†æå¥å­çš„è¯­ä¹‰ï¼ŒæŸ¥çœ‹æ˜¯å¦æ˜ç¡®æåˆ°äº†å®ä½“é—´çš„å…³ç³»
2. å¦‚æœå¥å­ä¸­æ²¡æœ‰æ˜ç¡®æåˆ°å…³ç³»ï¼Œè¯·åŸºäºä½ çš„è®¡ç®—æœºç§‘å­¦çŸ¥è¯†åº“å’Œä¸“ä¸šç†è§£ï¼Œè°¨æ…åˆ¤æ–­è¿™ä¸¤ä¸ªå®ä½“ä¹‹é—´æ˜¯å¦å­˜åœ¨åˆç†çš„å…³ç³»
3. åœ¨è¿›è¡ŒçŸ¥è¯†æ¨ç†æ—¶ï¼Œè¯·è€ƒè™‘ï¼š
   - è®¡ç®—æœºç§‘å­¦é¢†åŸŸçš„å¸¸è§å…³ç³»æ¨¡å¼
   - å®ä½“ç±»å‹ä¹‹é—´çš„å…¸å‹å…³è”
   - ä¸Šä¸‹æ–‡è¯­å¢ƒä¸­çš„éšå«å…³ç³»
   - ä¸“ä¸šé¢†åŸŸçš„æ ‡å‡†å®è·µå’Œæƒ¯ä¾‹
4. åˆ¤æ–­æ ‡å‡†ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰ï¼š
   - å¥å­æ˜ç¡®è¡¨è¿°çš„å…³ç³»ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
   - åŸºäºä¸“ä¸šçŸ¥è¯†çš„åˆç†æ¨ç†ï¼ˆä¸­ç­‰ä¼˜å…ˆçº§ï¼‰
   - å®ä½“ç±»å‹é—´çš„å…¸å‹å…³ç³»ï¼ˆè¾ƒä½ä¼˜å…ˆçº§ï¼‰
5. ç½®ä¿¡åº¦è®¾ç½®æŒ‡å¯¼ï¼š
   - å¥å­æ˜ç¡®æåˆ°å…³ç³»ï¼š0.8-1.0
   - åŸºäºä¸“ä¸šçŸ¥è¯†çš„å¼ºæ¨ç†ï¼š0.6-0.8
   - åŸºäºå¸¸è§æ¨¡å¼çš„æ¨ç†ï¼š0.4-0.6
   - ä¸ç¡®å®šæˆ–æ— å…³ç³»ï¼š0.0-0.4
6. åªæœ‰åœ¨å®Œå…¨æ— æ³•å»ºç«‹åˆç†å…³ç³»æ—¶æ‰è¿”å›"none"

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼Œä¸è¦åŒ…å«å…¶ä»–ä»»ä½•æ–‡æœ¬:
{{
    "relation": "å…³ç³»åç§°æˆ–none",
    "confidence": 0.0-1.0ä¹‹é—´çš„ç½®ä¿¡åº¦
}}
"""
        return prompt
    
    def get_cache_key(self, prompt):
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.md5(prompt.encode('utf-8')).hexdigest()
    
    def call_model_api(self, prompt):
        """è°ƒç”¨æ¨¡å‹APIï¼ˆå¸¦ç¼“å­˜å’Œé‡è¯•æœºåˆ¶ï¼‰"""
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self.get_cache_key(prompt)
        with self.cache_lock:
            if cache_key in self.cache:
                return self.cache[cache_key]
        
        # å‡†å¤‡APIè°ƒç”¨å‚æ•°
        messages = [{"role": "user", "content": prompt}]
        
        # é‡è¯•æœºåˆ¶
        for attempt in range(self.retry_attempts):
            try:
                response = self.client.chat.completions.create(
                    model=self.model_id,
                    messages=messages,
                    max_tokens=1024,
                    temperature=0.3,  # é™ä½éšæœºæ€§ï¼Œæé«˜ä¸€è‡´æ€§
                    top_p=0.9,
                    stream=False,
                    timeout=self.api_timeout
                )
                
                if response.choices and response.choices[0].message:
                    content = response.choices[0].message.content
                    
                    # ç¼“å­˜å“åº”
                    with self.cache_lock:
                        self.cache[cache_key] = content
                    
                    self.save_cache()
                    return content
                else:
                    print(f"  âš ï¸ APIå“åº”æ ¼å¼å¼‚å¸¸")
                    return None
                    
            except Exception as e:
                error_msg = str(e)
                print(f"  âŒ APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{self.retry_attempts}): {error_msg}")
                
                if "timeout" in error_msg.lower() or "timed out" in error_msg.lower():
                    wait_time = self.retry_delay * (2 ** attempt)
                    print(f"  â³ æ£€æµ‹åˆ°è¶…æ—¶é”™è¯¯ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                elif "rate limit" in error_msg.lower() or "429" in error_msg:
                    wait_time = self.retry_delay * (3 ** attempt)
                    print(f"  â³ æ£€æµ‹åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    wait_time = self.retry_delay * (attempt + 1)
                    if attempt < self.retry_attempts - 1:
                        time.sleep(wait_time)
                
                if attempt >= self.retry_attempts - 1:
                    print(f"  âŒ æ‰€æœ‰é‡è¯•å‡å¤±è´¥")
                    return None
    
    def parse_model_response(self, response):
        """è§£æå¤§æ¨¡å‹è¿”å›çš„JSONå­—ç¬¦ä¸²"""
        if not response:
            return None
        
        # å°è¯•æå–JSON
        json_match = re.search(r'\{[\s\S]*\}', response)
        if json_match:
            try:
                return json.loads(json_match.group(0))
            except:
                pass
        
        # å°è¯•ç›´æ¥è§£æ
        try:
            return json.loads(response)
        except:
            return None
    
    def process_single_item(self, item_info):
        """å¤„ç†å•ä¸ªå®ä½“å¯¹ï¼ˆç”¨äºå¹¶å‘ï¼‰"""
        i, item, total = item_info
        data_index = i - 1  # è½¬æ¢ä¸º0åŸºç´¢å¼•
        
        print(f"å¤„ç†å®ä½“å¯¹ {i}/{total}: {item['head']} -> {item['tail']}")
        
        # æ£€æŸ¥å®ä½“ç±»å‹
        head_type = self.get_entity_type(item['head'])
        tail_type = self.get_entity_type(item['tail'])
        
        if head_type == "Unknown" or tail_type == "Unknown":
            print(f"  âš ï¸ è·³è¿‡æœªçŸ¥å®ä½“ç±»å‹: {item['head']}({head_type}) -> {item['tail']}({tail_type})")
            result = dict(item)
            result['relation'] = 'none'
            result['confidence'] = 0.0
            result['reasoning'] = 'æœªçŸ¥å®ä½“ç±»å‹'
            result['data_index'] = data_index  # æ·»åŠ æ•°æ®ç´¢å¼•
            return result
        
        # ç”Ÿæˆæç¤ºè¯
        prompt = self.generate_prompt(item)
        if not prompt:
            print(f"  âš ï¸ æ²¡æœ‰å¯ç”¨å…³ç³»è§„åˆ™: {head_type} -> {tail_type}")
            result = dict(item)
            result['relation'] = 'none'
            result['confidence'] = 0.0
            result['reasoning'] = 'æ²¡æœ‰å¯ç”¨å…³ç³»è§„åˆ™'
            result['data_index'] = data_index  # æ·»åŠ æ•°æ®ç´¢å¼•
            return result
        
        # è°ƒç”¨API
        response = self.call_model_api(prompt)
        
        # è§£æå“åº”
        parsed = self.parse_model_response(response)
        
        # æ„å»ºç»“æœ
        result = dict(item)
        result['data_index'] = data_index  # æ·»åŠ æ•°æ®ç´¢å¼•
        if parsed and isinstance(parsed, dict):
            result['relation'] = parsed.get('relation', 'none')
            result['confidence'] = parsed.get('confidence', 0.0)
            result['reasoning'] = parsed.get('reasoning', 'æ— æ¨ç†è¯´æ˜')
        else:
            print(f"  âš ï¸ è§£æå“åº”å¤±è´¥")
            result['relation'] = 'none'
            result['confidence'] = 0.0
            result['reasoning'] = 'è§£æå“åº”å¤±è´¥'
        
        print(f"  âœ“ æ ‡æ³¨ç»“æœ: {result['relation']} (ç½®ä¿¡åº¦: {result['confidence']:.2f})")
        
        # ç«‹å³ä¿å­˜åˆ°CSVæ–‡ä»¶
        self.save_single_result_to_csv(result)
        
        return result
    
    def save_single_result_to_csv(self, result):
        """å°†å•æ¡ç»“æœç«‹å³ä¿å­˜åˆ°CSVæ–‡ä»¶"""
        try:
            # åªä¿å­˜æœ‰å…³ç³»çš„ç»“æœ
            if result.get('relation', 'none') != 'none':
                with open(self.csv_output_file, 'a', encoding='utf-8', newline='') as f:
                    writer = csv.writer(f)
                    writer.writerow([
                        result.get('sentence', ''),
                        result.get('relation', 'none'),
                        result.get('head', ''),
                        result.get('head_offset', ''),
                        result.get('tail', ''),
                        result.get('tail_offset', '')
                    ])
                print(f"  ğŸ’¾ å·²ä¿å­˜åˆ°CSV: {result['head']} -> {result['tail']} ({result['relation']})")
            
            # æ›´æ–°è¿›åº¦ï¼ˆæ— è®ºæ˜¯å¦æœ‰å…³ç³»éƒ½è¦è®°å½•å·²å¤„ç†ï¼‰
            if 'data_index' in result:
                self.processed_indices.add(result['data_index'])
                self.save_progress()
                
        except Exception as e:
            print(f"  âš ï¸ CSVä¿å­˜å¤±è´¥: {e}")
    
    def annotate_all(self):
        """æ ‡æ³¨æ‰€æœ‰å®ä½“å¯¹"""
        print(f"\nğŸ”„ å¼€å§‹è‡ªåŠ¨æ ‡æ³¨ {len(self.data)} æ¡æ•°æ®...")
        
        # æ˜¾ç¤ºæ–­ç‚¹ç»­ç”¨ä¿¡æ¯
        if len(self.processed_indices) > 0:
            remaining_count = len(self.data) - len(self.processed_indices)
            print(f"ğŸ“‹ æ£€æµ‹åˆ°æ–­ç‚¹ç»­ç”¨:")
            print(f"   - å·²å¤„ç†: {len(self.processed_indices)} æ¡")
            print(f"   - å‰©ä½™å¾…å¤„ç†: {remaining_count} æ¡")
        
        results = []
        total = len(self.data)
        
        # å‡†å¤‡æ•°æ®ï¼Œè¿‡æ»¤æ‰å·²å¤„ç†çš„é¡¹ç›®
        item_infos = []
        for i, item in enumerate(self.data):
            if i not in self.processed_indices:  # åªå¤„ç†æœªå¤„ç†çš„æ•°æ®
                item_infos.append((i+1, item, total))
        
        if not item_infos:
            print(f"âœ… æ‰€æœ‰æ•°æ®å·²å¤„ç†å®Œæˆï¼")
            self.show_statistics()
            self.show_csv_statistics()
            return []
        
        print(f"ğŸ”„ æœ¬æ¬¡éœ€è¦å¤„ç† {len(item_infos)} æ¡æ•°æ®")
        
        # æ‰¹æ¬¡å¤„ç†ä»¥å‡å°‘APIå‹åŠ›
        batch_size = 40
        
        for i in range(0, len(item_infos), batch_size):
            batch = item_infos[i:i+batch_size]
            print(f"\nğŸ”„ å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(item_infos) + batch_size - 1)//batch_size}")
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_item = {
                    executor.submit(self.process_single_item, item_info): item_info 
                    for item_info in batch
                }
                
                batch_results = []
                for future in concurrent.futures.as_completed(future_to_item):
                    try:
                        result = future.result()
                        batch_results.append(result)
                    except Exception as e:
                        item_info = future_to_item[future]
                        print(f"  âŒ å¤„ç†å¤±è´¥: {item_info[1]['head']} -> {item_info[1]['tail']} - {e}")
                
                results.extend(batch_results)
                
                # æ‰¹æ¬¡é—´ä¼‘æ¯
                if i + batch_size < len(item_infos):
                    print(f"  ğŸ’¤ æ‰¹æ¬¡å®Œæˆï¼Œä¼‘æ¯1ç§’...")
                    time.sleep(1)
        
        self.annotated_data = results
        print(f"\nâœ… æ ‡æ³¨å®Œæˆï¼Œæœ¬æ¬¡å¤„ç† {len(results)} æ¡æ•°æ®")
        print(f"ğŸ“Š æ€»è¿›åº¦: {len(self.processed_indices)}/{total} æ¡æ•°æ®å·²å®Œæˆ")
        
        # ç»Ÿè®¡ç»“æœ
        self.show_statistics()
        
        # æ˜¾ç¤ºCSVæ–‡ä»¶ç»Ÿè®¡
        self.show_csv_statistics()
        
        return results
    
    def show_csv_statistics(self):
        """æ˜¾ç¤ºCSVæ–‡ä»¶ç»Ÿè®¡"""
        try:
            with open(self.csv_output_file, 'r', encoding='utf-8') as f:
                reader = csv.reader(f)
                rows = list(reader)
                
            # å‡å»è¡¨å¤´
            data_rows = len(rows) - 1 if len(rows) > 1 else 0
            
            print(f"\nğŸ“„ CSVæ–‡ä»¶ç»Ÿè®¡:")
            print(f"   æ–‡ä»¶è·¯å¾„: {self.csv_output_file}")
            print(f"   ä¿å­˜çš„å…³ç³»æ•°æ®: {data_rows} æ¡")
            
            if data_rows > 0:
                # ç»Ÿè®¡å…³ç³»ç±»å‹
                relation_counts = {}
                for row in rows[1:]:  # è·³è¿‡è¡¨å¤´
                    if len(row) >= 2:
                        relation = row[1]
                        relation_counts[relation] = relation_counts.get(relation, 0) + 1
                
                print(f"   å…³ç³»ç±»å‹åˆ†å¸ƒ:")
                for relation, count in sorted(relation_counts.items(), key=lambda x: x[1], reverse=True):
                    print(f"     â€¢ {relation}: {count} æ¡")
                    
        except Exception as e:
            print(f"âš ï¸ CSVç»Ÿè®¡å¤±è´¥: {e}")
    
    def show_statistics(self):
        """æ˜¾ç¤ºæ ‡æ³¨ç»Ÿè®¡"""
        if not self.annotated_data:
            print("âŒ æ²¡æœ‰æ ‡æ³¨æ•°æ®")
            return
        
        print(f"\nğŸ“Š æ ‡æ³¨ç»Ÿè®¡:")
        
        # å…³ç³»åˆ†å¸ƒ
        relation_count = {}
        confidence_sum = {}
        for item in self.annotated_data:
            relation = item.get('relation', 'none')
            confidence = item.get('confidence', 0.0)
            
            relation_count[relation] = relation_count.get(relation, 0) + 1
            confidence_sum[relation] = confidence_sum.get(relation, 0) + confidence
        
        print(f"å…³ç³»åˆ†å¸ƒ:")
        for relation, count in sorted(relation_count.items(), key=lambda x: x[1], reverse=True):
            avg_confidence = confidence_sum[relation] / count if count > 0 else 0
            print(f"  â€¢ {relation}: {count} æ¡ (å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f})")
        
        # é«˜ç½®ä¿¡åº¦å…³ç³»
        high_confidence_relations = [
            item for item in self.annotated_data 
            if item.get('confidence', 0) >= 0.8 and item.get('relation') != 'none'
        ]
        print(f"\né«˜ç½®ä¿¡åº¦å…³ç³» (â‰¥0.8): {len(high_confidence_relations)} æ¡")
        
        # ä½ç½®ä¿¡åº¦å…³ç³»
        low_confidence_relations = [
            item for item in self.annotated_data 
            if 0 < item.get('confidence', 0) < 0.5 and item.get('relation') != 'none'
        ]
        print(f"ä½ç½®ä¿¡åº¦å…³ç³» (<0.5): {len(low_confidence_relations)} æ¡")
    
    def save_results(self):
        """ä¿å­˜æ ‡æ³¨ç»“æœ"""
        if not self.annotated_data:
            print("âŒ æ²¡æœ‰æ ‡æ³¨æ•°æ®å¯ä¿å­˜")
            return False
        
        try:
            with open(self.output_file, 'w', encoding='utf-8') as f:
                json.dump(self.annotated_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… æ ‡æ³¨ç»“æœå·²ä¿å­˜åˆ°: {self.output_file}")
            print(f"   æ€»æ¡æ•°: {len(self.annotated_data)}")
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def export_csv(self):
        """å¯¼å‡ºCSVæ ¼å¼"""
        if not self.annotated_data:
            print("âŒ æ²¡æœ‰æ ‡æ³¨æ•°æ®å¯å¯¼å‡º")
            return False
        
        csv_file = self.output_file.replace('.json', '.csv')
        
        try:
            with open(csv_file, 'w', encoding='utf-8', newline='') as f:
                fieldnames = ['sentence', 'head', 'tail', 'head_offset', 'tail_offset', 
                             'relation', 'confidence', 'reasoning']
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                
                for item in self.annotated_data:
                    writer.writerow({
                        'sentence': item.get('sentence', ''),
                        'head': item.get('head', ''),
                        'tail': item.get('tail', ''),
                        'head_offset': item.get('head_offset', ''),
                        'tail_offset': item.get('tail_offset', ''),
                        'relation': item.get('relation', 'none'),
                        'confidence': item.get('confidence', 0.0),
                        'reasoning': item.get('reasoning', '')
                    })
            
            print(f"âœ… CSVæ–‡ä»¶å·²å¯¼å‡ºåˆ°: {csv_file}")
            return True
        except Exception as e:
            print(f"âŒ CSVå¯¼å‡ºå¤±è´¥: {e}")
            return False
    
    def run(self):
        """æ‰§è¡Œå®Œæ•´æµç¨‹"""
        try:
            print("ğŸš€ å¼€å§‹è‡ªåŠ¨å…³ç³»æ ‡æ³¨...")
            
            # 1. æ ‡æ³¨æ‰€æœ‰æ•°æ®
            self.annotate_all()
            
            # 2. ä¿å­˜ç»“æœ
            self.save_results()
            
            # 3. å¯¼å‡ºCSV
            self.export_csv()
            
            # 4. æœ€ç»ˆä¿å­˜ç¼“å­˜
            self.save_cache()
            
            print("\nğŸ‰ è‡ªåŠ¨å…³ç³»æ ‡æ³¨å®Œæˆï¼")
            print(f"âœ… JSONç»“æœ: {self.output_file}")
            print(f"âœ… CSVç»“æœ: {self.csv_output_file}")
            print(f"ğŸ“ CSVæ ¼å¼ç¬¦åˆDeepKEæ ‡å‡†ï¼Œå¯ç›´æ¥ç”¨äºå…³ç³»æŠ½å–è®­ç»ƒ")
            
        except Exception as e:
            print(f"\nğŸš¨ æ ‡æ³¨è¿‡ç¨‹å‡ºé”™: {str(e)}")
            return False
        
        return True


def main():
    """ä¸»å‡½æ•°"""
    try:
        annotator = AutoRelationAnnotator()
        annotator.run()
    except Exception as e:
        print(f"åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        return False
    
    return True


if __name__ == "__main__":
    main()