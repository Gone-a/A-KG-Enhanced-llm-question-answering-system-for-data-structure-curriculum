#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®ç»“æ„è¯¾ç¨‹çŸ¥è¯†å›¾è°±æ•°æ®ç”Ÿæˆå™¨ - æ–°ç‰ˆæœ¬
åŸºäºæ–°çš„å®ä½“è¡¨å’Œå…³ç³»è¡¨ç”Ÿæˆé«˜è´¨é‡çš„å…³ç³»æŠ½å–è®­ç»ƒæ•°æ®
å…³ç³»ç±»å‹: hasComplexity, uses, variantOf, appliesTo, provides, implementedAs, usedIn
"""
import os
import time
import concurrent.futures
import openai
from tqdm import tqdm
import random
import logging
import re
import json
import csv
import hashlib
from typing import List, Dict, Tuple, Set
from collections import defaultdict, Counter

# åˆå§‹åŒ–å…¨å±€logger
logger = logging.getLogger(__name__)

# ç¦ç”¨HTTPè¯·æ±‚æ—¥å¿—
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)
logging.getLogger("openai").setLevel(logging.WARNING)

# å¯¼å…¥é…ç½®ç®¡ç†å™¨
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from modules.config_manager import get_config_manager

# è·å–é…ç½®ç®¡ç†å™¨å®ä¾‹
config_manager = get_config_manager()
api_config = config_manager.get_api_config()
gen_config = config_manager.get_generation_config()

# ============================= ç»Ÿä¸€é…ç½® =============================
class Config:
    """ç»Ÿä¸€é…ç½®ç±»"""
    # APIé…ç½®
    API_KEY = api_config.get('ark_api_key')
    BASE_URL = api_config.get('base_url')
    MODEL = api_config.get('doubao_model_id')
    TIMEOUT = gen_config.get('timeout', 15)  # å‡å°‘è¶…æ—¶æ—¶é—´
    RETRY_COUNT = gen_config.get('retry_count', 3)  # å¢åŠ é‡è¯•æ¬¡æ•°
    DELAY_BETWEEN_REQUESTS = gen_config.get('delay', 0.1)  # å¢åŠ å»¶è¿Ÿï¼Œä»0.01ç§’å¢åŠ åˆ°0.1ç§’
    CONCURRENCY = gen_config.get('concurrency', 20)  # å¤§å¹…å‡å°‘å¹¶å‘æ•°ï¼Œä»50å‡å°‘åˆ°5
    BATCH_SIZE = gen_config.get('batch_size', 100)  # å‡å°‘æ‰¹å¤„ç†å¤§å°ï¼Œä»1000å‡å°‘åˆ°100
    
    # æ•°æ®ç”Ÿæˆé…ç½®
    NUM_RECORDS = gen_config.get('num_records', 21000)  # æ¯ä¸ªå…³ç³»3000æ¡ï¼Œ7ä¸ªå…³ç³»å…±21000æ¡
    RECORDS_PER_RELATION = NUM_RECORDS // 7  # æ¯ä¸ªå…³ç³»çš„ç›®æ ‡æ•°æ®é‡
    MAX_PROMPTS = 1000
    MIN_PROMPTS_PER_RELATION = 5
    SENTENCES_PER_API_CALL = 3  # æ–°å¢ï¼šæ¯æ¬¡APIè°ƒç”¨ç”Ÿæˆå¤šä¸ªå¥å­
    
    # ç¼“å­˜é…ç½®
    ENABLE_CACHE = True  # å¯ç”¨ç¼“å­˜
    CACHE_SIZE = 10000  # ç¼“å­˜å¤§å°
    
    # æ–‡ä»¶è·¯å¾„é…ç½®
    OUTPUT_FILE = gen_config.get('output_file')
    PROMPTS_FILE = gen_config.get('prompts_file')
    VOCAB_DICT_FILE = gen_config.get('vocab_dict_file')
    RELATION_FILE = gen_config.get('relation_file')
    STATE_FILE = gen_config.get('state_file')
    CACHE_FILE = gen_config.get('cache_file')  # ç¼“å­˜æ–‡ä»¶
    
    ANNOTATION_OUTPUT_DIR = gen_config.get('annotation_output_dir')
    TRAIN_FILE = "train_new.csv"
    TEST_FILE = "test_new.csv"
    VALID_FILE = "valid_new.csv"

# ========================= åŸºäºæ–°å®ä½“è¡¨çš„çŸ¥è¯†åº“ =============================
def load_entities_from_vocab():
    """ä»vocab_dict.csvåŠ è½½å®ä½“å’Œç±»å‹æ˜ å°„"""
    entities_by_type = defaultdict(list)
    entity_to_type = {}
    
    vocab_path = Config.VOCAB_DICT_FILE
    try:
        with open(vocab_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            for row in reader:
                if len(row) >= 2:
                    entity, entity_type = row[0].strip(), row[1].strip()
                    entities_by_type[entity_type].append(entity)
                    entity_to_type[entity] = entity_type
    except Exception as e:
        print(f"âŒ åŠ è½½å®ä½“è¯å…¸å¤±è´¥: {e}")
        return {}, {}
    
    return dict(entities_by_type), entity_to_type

# åŠ è½½å®ä½“æ•°æ®
ENTITIES_BY_TYPE, ENTITY_TO_TYPE = load_entities_from_vocab()

# ========================= æ–°å…³ç³»å®šä¹‰ =============================
RELATION_TYPES = {
    "hasComplexity": "ç®—æ³•çš„å¤æ‚åº¦å±æ€§",
    "uses": "ç®—æ³•ä½¿ç”¨çš„æ•°æ®ç»“æ„", 
    "variantOf": "æ•°æ®ç»“æ„çš„å˜ä½“å…³ç³»",
    "appliesTo": "æ•°æ®ç»“æ„çš„åº”ç”¨åœºæ™¯",
    "provides": "æ•°æ®ç»“æ„æä¾›çš„æ“ä½œ",
    "implementedAs": "æ•°æ®ç»“æ„çš„å®ç°æ–¹å¼",
    "usedIn": "æ“ä½œçš„åº”ç”¨åœºæ™¯"
}

# ========================= å…³ç³»æ¨¡æ¿ =============================
RELATION_TEMPLATES = {
    "hasComplexity": [
        "{algorithm}ç®—æ³•çš„æ—¶é—´å¤æ‚åº¦ä¸º{complexity}",
        "{algorithm}çš„å¹³å‡æ—¶é—´å¤æ‚åº¦æ˜¯{complexity}",
        "{algorithm}ç®—æ³•åœ¨æœ€åæƒ…å†µä¸‹çš„å¤æ‚åº¦ä¸º{complexity}",
        "{algorithm}çš„ç©ºé—´å¤æ‚åº¦ä¸º{complexity}",
        "ä½¿ç”¨{algorithm}è¿›è¡Œå¤„ç†ï¼Œå…¶å¤æ‚åº¦ä¸º{complexity}",
        "{algorithm}ç®—æ³•å…·æœ‰{complexity}çš„æ—¶é—´å¤æ‚åº¦",
        "åœ¨åˆ†æ{algorithm}æ—¶ï¼Œå‘ç°å…¶å¤æ‚åº¦ä¸º{complexity}",
        "{algorithm}çš„è®¡ç®—å¤æ‚åº¦è¾¾åˆ°{complexity}"
    ],
    "uses": [
        "{algorithm}ç®—æ³•éœ€è¦ä½¿ç”¨{datastructure}æ¥å®ç°",
        "{algorithm}çš„å®ç°ä¾èµ–äº{datastructure}æ•°æ®ç»“æ„",
        "åœ¨{algorithm}ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨{datastructure}æ¥å­˜å‚¨æ•°æ®",
        "{algorithm}ç®—æ³•é‡‡ç”¨{datastructure}ä½œä¸ºæ ¸å¿ƒæ•°æ®ç»“æ„",
        "å®ç°{algorithm}æ—¶ï¼Œ{datastructure}æ˜¯å¿…ä¸å¯å°‘çš„",
        "{algorithm}ç®—æ³•åŸºäº{datastructure}è¿›è¡Œæ“ä½œ",
        "ä¸ºäº†æ‰§è¡Œ{algorithm}ï¼Œç³»ç»Ÿä½¿ç”¨äº†{datastructure}",
        "{algorithm}çš„é«˜æ•ˆå®ç°éœ€è¦{datastructure}çš„æ”¯æŒ"
    ],
    "variantOf": [
        "{variant}æ˜¯{original}çš„ä¸€ç§å˜ä½“",
        "{variant}å±äº{original}çš„ç‰¹æ®Šå½¢å¼",
        "{variant}æ˜¯åŸºäº{original}æ”¹è¿›çš„æ•°æ®ç»“æ„",
        "ä½œä¸º{original}çš„å˜ä½“ï¼Œ{variant}å…·æœ‰æ›´å¥½çš„æ€§èƒ½",
        "{variant}æ˜¯{original}çš„ä¼˜åŒ–ç‰ˆæœ¬",
        "{variant}ç»§æ‰¿äº†{original}çš„åŸºæœ¬ç‰¹æ€§",
        "ä»{original}å‘å±•è€Œæ¥çš„{variant}å…·æœ‰ç‹¬ç‰¹ä¼˜åŠ¿",
        "{variant}æ˜¯{original}åœ¨ç‰¹å®šåœºæ™¯ä¸‹çš„å˜å½¢"
    ],
    "appliesTo": [
        "{datastructure}å¹¿æ³›åº”ç”¨äº{scenario}åœºæ™¯",
        "åœ¨{scenario}ä¸­ï¼Œ{datastructure}å‘æŒ¥é‡è¦ä½œç”¨",
        "{datastructure}ç‰¹åˆ«é€‚åˆç”¨äº{scenario}",
        "å½“éœ€è¦å¤„ç†{scenario}æ—¶ï¼Œ{datastructure}æ˜¯ç†æƒ³é€‰æ‹©",
        "{datastructure}åœ¨{scenario}æ–¹é¢è¡¨ç°å‡ºè‰²",
        "è§£å†³{scenario}é—®é¢˜æ—¶ï¼Œ{datastructure}éå¸¸æœ‰æ•ˆ",
        "{datastructure}æ˜¯{scenario}çš„æ ¸å¿ƒæ•°æ®ç»“æ„",
        "åœ¨{scenario}çš„å®ç°ä¸­ï¼Œ{datastructure}ä¸å¯æˆ–ç¼º"
    ],
    "provides": [
        "{datastructure}æä¾›äº†{operation}åŠŸèƒ½",
        "{datastructure}æ”¯æŒ{operation}æ“ä½œ",
        "é€šè¿‡{datastructure}å¯ä»¥å®ç°{operation}",
        "{datastructure}å…·å¤‡{operation}çš„èƒ½åŠ›",
        "ä½¿ç”¨{datastructure}èƒ½å¤Ÿè¿›è¡Œ{operation}",
        "{datastructure}å…è®¸ç”¨æˆ·æ‰§è¡Œ{operation}",
        "{datastructure}çš„æ ¸å¿ƒåŠŸèƒ½åŒ…æ‹¬{operation}",
        "åœ¨{datastructure}ä¸­ï¼Œ{operation}æ˜¯åŸºæœ¬æ“ä½œ"
    ],
    "implementedAs": [
        "{datastructure}å¯ä»¥é€šè¿‡{algorithm}æ¥å®ç°",
        "{datastructure}çš„å®ç°é‡‡ç”¨äº†{algorithm}æ–¹æ³•",
        "ä½¿ç”¨{algorithm}å¯ä»¥æ„å»º{datastructure}",
        "{datastructure}åŸºäº{algorithm}è¿›è¡Œå®ç°",
        "é€šè¿‡{algorithm}ï¼Œæˆ‘ä»¬å¯ä»¥å®ç°{datastructure}",
        "{datastructure}çš„åº•å±‚å®ç°ä½¿ç”¨{algorithm}",
        "{algorithm}æ˜¯å®ç°{datastructure}çš„æœ‰æ•ˆæ–¹å¼",
        "{datastructure}é‡‡ç”¨{algorithm}ä½œä¸ºå®ç°ç­–ç•¥"
    ],
    "usedIn": [
        "{operation}æ“ä½œå¸¸ç”¨äº{scenario}",
        "åœ¨{scenario}ä¸­ï¼Œ{operation}æ˜¯å…³é”®æ“ä½œ",
        "{operation}åœ¨{scenario}åœºæ™¯ä¸‹éå¸¸é‡è¦",
        "å¤„ç†{scenario}æ—¶éœ€è¦ä½¿ç”¨{operation}",
        "{operation}æ˜¯{scenario}çš„æ ¸å¿ƒæ“ä½œ",
        "å®ç°{scenario}åŠŸèƒ½éœ€è¦{operation}æ”¯æŒ",
        "{scenario}çš„å®ç°ç¦»ä¸å¼€{operation}æ“ä½œ",
        "åœ¨{scenario}è¿‡ç¨‹ä¸­ï¼Œ{operation}å‘æŒ¥é‡è¦ä½œç”¨"
    ]
}

# ========================= æ•°æ®ç”Ÿæˆå™¨ç±» =============================

class KnowledgeGraphGenerator:
    """çŸ¥è¯†å›¾è°±æ•°æ®ç”Ÿæˆå™¨"""
    
    def __init__(self, api_key):
        """åˆå§‹åŒ–çŸ¥è¯†å›¾è°±ç”Ÿæˆå™¨"""
        self.api_key = api_key
        
        # é…ç½®OpenAIå®¢æˆ·ç«¯ï¼ˆæ—§ç‰ˆæœ¬æ–¹å¼ï¼‰
        openai.api_key = api_key
        openai.api_base = "https://ark.cn-beijing.volces.com/api/v3"
        
        # åŠ è½½å®ä½“æ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        self.entities_by_type = self._load_entities_optimized()
        
        # é¢„è®¡ç®—å…³ç³»å¯¹ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        self.build_relation_pairs()
        
        # åˆå§‹åŒ–ç¼“å­˜
        self.api_cache = {}
        self.cache_file = Config.CACHE_FILE
        self.enable_cache = Config.ENABLE_CACHE
        self.cache_size = Config.CACHE_SIZE
        
        # åˆå§‹åŒ–å…³ç³»è®¡æ•°å™¨
        self.relation_counts = {relation: 0 for relation in RELATION_TYPES.keys()}
        
        # åˆå§‹åŒ–ç”Ÿæˆçš„å¥å­åˆ—è¡¨
        self.generated_sentences = []
        
        # åˆå§‹åŒ–è¾“å‡ºæ–‡ä»¶è·¯å¾„
        self.output_file = Config.OUTPUT_FILE
        
        # å®æ—¶ç¼“å­˜é…ç½®
        self.realtime_cache_file = Config.OUTPUT_FILE.replace('.txt', '_realtime.json')
        self.cache_save_interval = 50  # æ¯ç”Ÿæˆ50æ¡å¥å­ä¿å­˜ä¸€æ¬¡
        self.last_cache_save = 0
        
        if self.enable_cache:
            self.load_cache()
            self.load_realtime_cache()
        
        print(f"âœ… çŸ¥è¯†å›¾è°±ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“Š å®ä½“ç»Ÿè®¡: {sum(len(entities) for entities in self.entities_by_type.values())} ä¸ªå®ä½“")
        print(f"ğŸ”— å…³ç³»å¯¹ç»Ÿè®¡: {sum(len(pairs) for pairs in self.relation_pairs.values())} ä¸ªå…³ç³»å¯¹")
    
    def _load_entities_optimized(self):
        """ä¼˜åŒ–çš„å®ä½“åŠ è½½æ–¹æ³•"""
        print("ğŸ“š åŠ è½½å®ä½“æ•°æ®...")
        
        # ç›´æ¥ä½¿ç”¨é¢„å®šä¹‰çš„å®ä½“æ•°æ®ï¼Œé¿å…é‡å¤å¤„ç†
        entities_by_type = {
            "Algorithm": ENTITIES_BY_TYPE.get("Algorithm", []),
            "DataStructure": ENTITIES_BY_TYPE.get("DataStructure", []),
            "Complexity": ENTITIES_BY_TYPE.get("Complexity", []),
            "Operation": ENTITIES_BY_TYPE.get("Operation", []),
            "Scenario": ENTITIES_BY_TYPE.get("ApplicationScenario", [])
        }
        
        print("âœ… å®ä½“æ•°æ®åŠ è½½å®Œæˆ")
        return entities_by_type

    def build_relation_pairs(self):
        """æ„å»ºå…³ç³»å®ä½“å¯¹ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        print("ğŸ”— æ„å»ºå…³ç³»å®ä½“å¯¹...")
        
        # é¢„è®¡ç®—æ‰€æœ‰å¯èƒ½çš„å®ä½“å¯¹ï¼Œé¿å…è¿è¡Œæ—¶é‡å¤è®¡ç®—
        self.relation_pairs = {
            "hasComplexity": self._build_complexity_pairs(),
            "uses": self._build_uses_pairs(),
            "variantOf": self._build_variant_pairs(),
            "appliesTo": self._build_applies_pairs(),
            "provides": self._build_provides_pairs(),
            "implementedAs": self._build_implemented_pairs(),
            "usedIn": self._build_used_in_pairs()
        }
        
        print("âœ… å…³ç³»å®ä½“å¯¹æ„å»ºå®Œæˆ")
    
    def _build_complexity_pairs(self):
        """æ„å»ºç®—æ³•-å¤æ‚åº¦å…³ç³»å¯¹"""
        pairs = []
        algorithms = self.entities_by_type.get("Algorithm", [])
        complexities = self.entities_by_type.get("Complexity", [])
        
        # ä¸ºæ¯ä¸ªç®—æ³•åˆ†é…åˆé€‚çš„å¤æ‚åº¦
        for algorithm in algorithms:
            # éšæœºé€‰æ‹©1-2ä¸ªå¤æ‚åº¦
            selected_complexities = random.sample(complexities, min(2, len(complexities)))
            for complexity in selected_complexities:
                pairs.append((algorithm, complexity))
        
        return pairs[:Config.RECORDS_PER_RELATION]
    
    def _build_uses_pairs(self):
        """æ„å»ºç®—æ³•-æ•°æ®ç»“æ„å…³ç³»å¯¹"""
        pairs = []
        algorithms = self.entities_by_type.get("Algorithm", [])
        data_structures = self.entities_by_type.get("DataStructure", [])
        
        for algorithm in algorithms:
            # æ¯ä¸ªç®—æ³•ä½¿ç”¨1-3ä¸ªæ•°æ®ç»“æ„
            selected_ds = random.sample(data_structures, min(3, len(data_structures)))
            for ds in selected_ds:
                pairs.append((algorithm, ds))
        
        return pairs[:Config.RECORDS_PER_RELATION]
    
    def _build_variant_pairs(self):
        """æ„å»ºæ•°æ®ç»“æ„å˜ä½“å…³ç³»å¯¹"""
        pairs = []
        data_structures = self.entities_by_type.get("DataStructure", [])
        
        # åˆ›å»ºå˜ä½“å…³ç³»ï¼ˆä¸€äº›æ•°æ®ç»“æ„æ˜¯å…¶ä»–çš„å˜ä½“ï¼‰
        for i, ds1 in enumerate(data_structures):
            for j, ds2 in enumerate(data_structures):
                if i != j and random.random() < 0.1:  # 10%çš„æ¦‚ç‡åˆ›å»ºå˜ä½“å…³ç³»
                    pairs.append((ds1, ds2))
        
        return pairs[:Config.RECORDS_PER_RELATION]
    
    def _build_applies_pairs(self):
        """æ„å»ºæ•°æ®ç»“æ„-åº”ç”¨åœºæ™¯å…³ç³»å¯¹"""
        pairs = []
        data_structures = self.entities_by_type.get("DataStructure", [])
        scenarios = self.entities_by_type.get("Scenario", [])
        
        for ds in data_structures:
            # æ¯ä¸ªæ•°æ®ç»“æ„åº”ç”¨äº2-4ä¸ªåœºæ™¯
            selected_scenarios = random.sample(scenarios, min(4, len(scenarios)))
            for scenario in selected_scenarios:
                pairs.append((ds, scenario))
        
        return pairs[:Config.RECORDS_PER_RELATION]
    
    def _build_provides_pairs(self):
        """æ„å»ºæ•°æ®ç»“æ„-æ“ä½œå…³ç³»å¯¹"""
        pairs = []
        data_structures = self.entities_by_type.get("DataStructure", [])
        operations = self.entities_by_type.get("Operation", [])
        
        for ds in data_structures:
            # æ¯ä¸ªæ•°æ®ç»“æ„æä¾›3-5ä¸ªæ“ä½œ
            selected_ops = random.sample(operations, min(5, len(operations)))
            for op in selected_ops:
                pairs.append((ds, op))
        
        return pairs[:Config.RECORDS_PER_RELATION]
    
    def _build_implemented_pairs(self):
        """æ„å»ºæ•°æ®ç»“æ„-ç®—æ³•å®ç°å…³ç³»å¯¹"""
        pairs = []
        data_structures = self.entities_by_type.get("DataStructure", [])
        algorithms = self.entities_by_type.get("Algorithm", [])
        
        for ds in data_structures:
            # æ¯ä¸ªæ•°æ®ç»“æ„å¯ä»¥é€šè¿‡1-2ä¸ªç®—æ³•å®ç°
            selected_algs = random.sample(algorithms, min(2, len(algorithms)))
            for alg in selected_algs:
                pairs.append((ds, alg))
        
        return pairs[:Config.RECORDS_PER_RELATION]
    
    def _build_used_in_pairs(self):
        """æ„å»ºæ“ä½œ-åº”ç”¨åœºæ™¯å…³ç³»å¯¹"""
        pairs = []
        operations = self.entities_by_type.get("Operation", [])
        scenarios = self.entities_by_type.get("Scenario", [])
        
        for op in operations:
            # æ¯ä¸ªæ“ä½œç”¨äº2-3ä¸ªåœºæ™¯
            selected_scenarios = random.sample(scenarios, min(3, len(scenarios)))
            for scenario in selected_scenarios:
                pairs.append((op, scenario))
        
        return pairs[:Config.RECORDS_PER_RELATION]
    
    def load_cache(self):
        """åŠ è½½APIç¼“å­˜"""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    self.api_cache = json.load(f)
                print(f"ğŸ“¦ åŠ è½½ç¼“å­˜: {len(self.api_cache)} æ¡è®°å½•")
            else:
                self.api_cache = {}
        except Exception as e:
            logger.warning(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            self.api_cache = {}
    
    def save_cache(self):
        """ä¿å­˜APIç¼“å­˜"""
        if not self.enable_cache:
            return
        
        try:
            # é™åˆ¶ç¼“å­˜å¤§å°
            if len(self.api_cache) > self.cache_size:
                # ä¿ç•™æœ€æ–°çš„ç¼“å­˜é¡¹
                items = list(self.api_cache.items())
                self.api_cache = dict(items[-self.cache_size:])
            
            os.makedirs(os.path.dirname(self.cache_file), exist_ok=True)
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.api_cache, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ä¿å­˜ç¼“å­˜: {len(self.api_cache)} æ¡è®°å½•")
        except Exception as e:
            logger.warning(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def load_realtime_cache(self):
        """åŠ è½½å®æ—¶ç¼“å­˜çš„ç”Ÿæˆç»“æœ"""
        try:
            if os.path.exists(self.realtime_cache_file):
                with open(self.realtime_cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.generated_sentences = data.get('sentences', [])
                    self.relation_counts = data.get('relation_counts', {relation: 0 for relation in RELATION_TYPES.keys()})
                print(f"ğŸ“¦ åŠ è½½å®æ—¶ç¼“å­˜: {len(self.generated_sentences)} æ¡å¥å­")
            else:
                self.generated_sentences = []
                self.relation_counts = {relation: 0 for relation in RELATION_TYPES.keys()}
        except Exception as e:
            logger.warning(f"åŠ è½½å®æ—¶ç¼“å­˜å¤±è´¥: {e}")
            self.generated_sentences = []
            self.relation_counts = {relation: 0 for relation in RELATION_TYPES.keys()}
    
    def save_realtime_cache(self, force=False):
        """ä¿å­˜å®æ—¶ç¼“å­˜"""
        if not self.enable_cache:
            return
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿å­˜
        current_count = len(self.generated_sentences)
        if not force and current_count - self.last_cache_save < self.cache_save_interval:
            return
        
        try:
            os.makedirs(os.path.dirname(self.realtime_cache_file), exist_ok=True)
            data = {
                'sentences': self.generated_sentences,
                'relation_counts': self.relation_counts,
                'timestamp': time.time(),
                'total_count': current_count
            }
            with open(self.realtime_cache_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            self.last_cache_save = current_count
            print(f"ğŸ’¾ å®æ—¶ç¼“å­˜å·²ä¿å­˜: {current_count} æ¡å¥å­")
        except Exception as e:
            logger.warning(f"ä¿å­˜å®æ—¶ç¼“å­˜å¤±è´¥: {e}")
    
    def get_cache_key(self, prompt):
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.md5(prompt.encode('utf-8')).hexdigest()
    
    def generate_prompt_for_relation(self, relation, entity1, entity2, num_sentences=1):
        """ä¸ºç‰¹å®šå…³ç³»ç”Ÿæˆæç¤ºè¯"""
        relation_desc = RELATION_TYPES[relation]
        
        # æ ¹æ®å…³ç³»ç±»å‹é€‰æ‹©åˆé€‚çš„æ¨¡æ¿
        templates = RELATION_TEMPLATES.get(relation, [])
        if not templates:
            return f"è¯·ç”Ÿæˆä¸€ä¸ªæè¿°{entity1}å’Œ{entity2}ä¹‹é—´{relation_desc}å…³ç³»çš„å¥å­ã€‚"
        
        # éšæœºé€‰æ‹©æ¨¡æ¿
        template = random.choice(templates)
        
        # æ ¹æ®å…³ç³»ç±»å‹å¡«å……æ¨¡æ¿
        if relation == "hasComplexity":
            example = template.format(algorithm=entity1, complexity=entity2)
        elif relation == "uses":
            example = template.format(algorithm=entity1, datastructure=entity2)
        elif relation == "variantOf":
            example = template.format(variant=entity1, original=entity2)
        elif relation == "appliesTo":
            example = template.format(datastructure=entity1, scenario=entity2)
        elif relation == "provides":
            example = template.format(datastructure=entity1, operation=entity2)
        elif relation == "implementedAs":
            example = template.format(datastructure=entity1, algorithm=entity2)
        elif relation == "usedIn":
            example = template.format(operation=entity1, scenario=entity2)
        else:
            example = f"{entity1}ä¸{entity2}å­˜åœ¨{relation_desc}å…³ç³»"
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†å›¾è°±æ•°æ®ç”Ÿæˆä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆ{num_sentences}ä¸ªé«˜è´¨é‡çš„ä¸­æ–‡å¥å­ï¼š

å…³ç³»ç±»å‹ï¼š{relation} ({relation_desc})
å®ä½“1ï¼š{entity1}
å®ä½“2ï¼š{entity2}

å‚è€ƒç¤ºä¾‹ï¼š{example}

è¦æ±‚ï¼š
1. ç”Ÿæˆ{num_sentences}ä¸ªä¸åŒçš„å¥å­ï¼Œæ¯ä¸ªå¥å­ä¸€è¡Œ
2. å¥å­è¦è‡ªç„¶æµç•…ï¼Œç¬¦åˆä¸­æ–‡è¡¨è¾¾ä¹ æƒ¯
3. å‡†ç¡®ä½“ç°{entity1}å’Œ{entity2}ä¹‹é—´çš„{relation_desc}å…³ç³»
4. å¥å­é•¿åº¦é€‚ä¸­ï¼ˆ10-30ä¸ªå­—ç¬¦ï¼‰
5. é¿å…é‡å¤å’Œå†—ä½™è¡¨è¾¾
6. åªè¾“å‡ºå¥å­å†…å®¹ï¼Œä¸è¦æ·»åŠ ç¼–å·æˆ–å…¶ä»–æ ‡è®°

è¯·ç›´æ¥è¾“å‡º{num_sentences}ä¸ªå¥å­ï¼š"""
        
        return prompt

    def call_api_batch(self, prompts):
        """æ‰¹é‡è°ƒç”¨API"""
        if not prompts:
            return []
        
        responses = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=Config.CONCURRENCY) as executor:
            future_to_prompt = {executor.submit(self.call_api, prompt): prompt for prompt in prompts}
            
            for future in concurrent.futures.as_completed(future_to_prompt):
                try:
                    response = future.result()
                    responses.append(response)
                except Exception as e:
                    logger.error(f"APIè°ƒç”¨å¤±è´¥: {e}")
                    responses.append(None)
                
                # æ§åˆ¶è¯·æ±‚é¢‘ç‡
                time.sleep(Config.DELAY_BETWEEN_REQUESTS)
        
        return responses
    
    def call_api(self, prompt):
        """è°ƒç”¨APIç”Ÿæˆæ–‡æœ¬ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        # æ£€æŸ¥ç¼“å­˜
        if self.enable_cache:
            cache_key = self.get_cache_key(prompt)
            if cache_key in self.api_cache:
                return self.api_cache[cache_key]
        
        for attempt in range(Config.RETRY_COUNT):
            try:
                response = openai.ChatCompletion.create(
                    model=Config.MODEL,
                    messages=[
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.7,
                    max_tokens=500,
                    timeout=Config.TIMEOUT
                )
                
                result = response.choices[0].message.content.strip()
                
                # ä¿å­˜åˆ°ç¼“å­˜
                if self.enable_cache:
                    self.api_cache[cache_key] = result
                
                return result
                
            except Exception as e:
                logger.warning(f"APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{Config.RETRY_COUNT}): {e}")
                if attempt < Config.RETRY_COUNT - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                else:
                    logger.error(f"APIè°ƒç”¨æœ€ç»ˆå¤±è´¥: {e}")
                    return None
    
    def process_api_response(self, response, relation, entity1, entity2):
        """å¤„ç†APIå“åº”ï¼Œæå–æœ‰æ•ˆå¥å­"""
        if not response:
            return []
        
        sentences = []
        lines = response.split('\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # æ¸…ç†å¥å­æ ¼å¼
            line = re.sub(r'^\d+[\.\)]\s*', '', line)  # ç§»é™¤åºå·
            line = re.sub(r'^[-\*]\s*', '', line)      # ç§»é™¤åˆ—è¡¨ç¬¦å·
            line = line.strip('\"\'')                   # ç§»é™¤å¼•å·
            
            # æ£€æŸ¥å¥å­è´¨é‡
            if (len(line) >= 10 and len(line) <= 100 and 
                entity1 in line and entity2 in line):
                sentences.append({
                    'sentence': line,
                    'relation': relation,
                    'entity1': entity1,
                    'entity2': entity2
                })
        
        return sentences
    
    def generate_mock_sentences_for_relation(self, relation, target_count):
        """ä¸ºç‰¹å®šå…³ç³»ç”Ÿæˆæ¨¡æ‹Ÿå¥å­ï¼ˆç”¨äºæµ‹è¯•ï¼Œä¸è°ƒç”¨APIï¼‰"""
        print(f"\nğŸ”„ ç”Ÿæˆ {relation} å…³ç³»çš„æ¨¡æ‹Ÿå¥å­ (ç›®æ ‡: {target_count} æ¡)")
        
        pairs = self.relation_pairs[relation]
        if not pairs:
            print(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ° {relation} å…³ç³»çš„å®ä½“å¯¹")
            return []
        
        generated_sentences = []
        templates = RELATION_TEMPLATES.get(relation, [])
        
        if not templates:
            print(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ° {relation} å…³ç³»çš„æ¨¡æ¿")
            return []
        
        with tqdm(total=target_count, desc=f"ç”Ÿæˆ{relation}") as pbar:
            while len(generated_sentences) < target_count:
                # éšæœºé€‰æ‹©å®ä½“å¯¹å’Œæ¨¡æ¿
                entity1, entity2 = random.choice(pairs)
                template = random.choice(templates)
                
                # æ ¹æ®å…³ç³»ç±»å‹å¡«å……æ¨¡æ¿
                if relation == "hasComplexity":
                    sentence = template.format(algorithm=entity1, complexity=entity2)
                elif relation == "uses":
                    sentence = template.format(algorithm=entity1, datastructure=entity2)
                elif relation == "variantOf":
                    sentence = template.format(variant=entity1, original=entity2)
                elif relation == "appliesTo":
                    sentence = template.format(datastructure=entity1, scenario=entity2)
                elif relation == "provides":
                    sentence = template.format(datastructure=entity1, operation=entity2)
                elif relation == "implementedAs":
                    sentence = template.format(datastructure=entity1, algorithm=entity2)
                elif relation == "usedIn":
                    sentence = template.format(operation=entity1, scenario=entity2)
                else:
                    sentence = f"{entity1}ä¸{entity2}å­˜åœ¨{relation}å…³ç³»"
                
                generated_sentences.append({
                    'sentence': sentence,
                    'relation': relation,
                    'entity1': entity1,
                    'entity2': entity2
                })
                pbar.update(1)
        
        print(f"âœ… {relation} å…³ç³»ç”Ÿæˆå®Œæˆ: {len(generated_sentences)} æ¡")
        return generated_sentences

    def generate_sentences_for_relation_fast(self, relation, target_count):
        """é«˜æ•ˆç”Ÿæˆç‰¹å®šå…³ç³»çš„å¥å­ï¼ˆå¹¶å‘+æ‰¹é‡ï¼‰"""
        print(f"\nğŸš€ é«˜æ•ˆç”Ÿæˆ {relation} å…³ç³»çš„å¥å­ (ç›®æ ‡: {target_count} æ¡)")
        
        pairs = self.relation_pairs[relation]
        if not pairs:
            print(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ° {relation} å…³ç³»çš„å®ä½“å¯¹")
            return []
        
        generated_sentences = []
        sentences_per_call = Config.SENTENCES_PER_API_CALL
        
        # è®¡ç®—éœ€è¦çš„APIè°ƒç”¨æ¬¡æ•°
        calls_needed = (target_count + sentences_per_call - 1) // sentences_per_call
        
        # å‡†å¤‡æ‰¹é‡æç¤ºè¯
        prompts = []
        for i in range(calls_needed):
            entity1, entity2 = random.choice(pairs)
            remaining = min(sentences_per_call, target_count - len(generated_sentences))
            if remaining <= 0:
                break
            prompt = self.generate_prompt_for_relation(relation, entity1, entity2, remaining)
            prompts.append((prompt, relation, entity1, entity2, remaining))
        
        print(f"ğŸ“Š å‡†å¤‡è¿›è¡Œ {len(prompts)} æ¬¡APIè°ƒç”¨ï¼Œæ¯æ¬¡ç”Ÿæˆ {sentences_per_call} ä¸ªå¥å­")
        
        # æ‰¹é‡å¤„ç†æç¤ºè¯
        batch_size = Config.BATCH_SIZE // sentences_per_call  # è°ƒæ•´æ‰¹æ¬¡å¤§å°
        
        with tqdm(total=target_count, desc=f"ç”Ÿæˆ{relation}") as pbar:
            for i in range(0, len(prompts), batch_size):
                batch_prompts = prompts[i:i + batch_size]
                
                # å¹¶å‘è°ƒç”¨API
                with concurrent.futures.ThreadPoolExecutor(max_workers=min(Config.CONCURRENCY, len(batch_prompts))) as executor:
                    future_to_data = {
                        executor.submit(self.call_api, prompt_data[0]): prompt_data 
                        for prompt_data in batch_prompts
                    }
                    
                    for future in concurrent.futures.as_completed(future_to_data):
                        prompt_data = future_to_data[future]
                        _, relation, entity1, entity2, expected_count = prompt_data
                        
                        try:
                            response = future.result()
                            if response:
                                sentences = self.process_api_response(response, relation, entity1, entity2)
                                for sentence in sentences[:expected_count]:
                                    if len(generated_sentences) < target_count:
                                        generated_sentences.append(sentence)
                                        # æ·»åŠ åˆ°å®æ—¶ç¼“å­˜
                                        self.generated_sentences.append(sentence)
                                        pbar.update(1)
                        except Exception as e:
                            logger.error(f"å¤„ç†APIå“åº”å¤±è´¥: {e}")
                        
                        # æ§åˆ¶è¯·æ±‚é¢‘ç‡
                        time.sleep(Config.DELAY_BETWEEN_REQUESTS)
                
                # å®šæœŸä¿å­˜å®æ—¶ç¼“å­˜
                self.save_realtime_cache()
                
                # å¦‚æœå·²ç»ç”Ÿæˆè¶³å¤Ÿçš„å¥å­ï¼Œæå‰é€€å‡º
                if len(generated_sentences) >= target_count:
                    break
        
        print(f"âœ… {relation} å…³ç³»ç”Ÿæˆå®Œæˆ: {len(generated_sentences)} æ¡")
        return generated_sentences[:target_count]
    
    def generate_all_data_fast(self):
        """é«˜æ•ˆç”Ÿæˆæ‰€æœ‰å…³ç³»çš„æ•°æ®ï¼ˆå¹¶å‘å¤„ç†ä¸åŒå…³ç³»ï¼‰"""
        print(f"\nğŸš€ å¼€å§‹é«˜æ•ˆç”ŸæˆçŸ¥è¯†å›¾è°±æ•°æ®")
        print(f"ç›®æ ‡: æ¯ä¸ªå…³ç³» {Config.RECORDS_PER_RELATION} æ¡ï¼Œå…± {len(RELATION_TYPES)} ä¸ªå…³ç³»")
        
        all_sentences = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†ä¸åŒå…³ç³»
        with concurrent.futures.ThreadPoolExecutor(max_workers=min(len(RELATION_TYPES), 4)) as executor:
            future_to_relation = {
                executor.submit(self.generate_sentences_for_relation_fast, relation, Config.RECORDS_PER_RELATION): relation
                for relation in RELATION_TYPES.keys()
            }
            
            for future in concurrent.futures.as_completed(future_to_relation):
                relation = future_to_relation[future]
                try:
                    sentences = future.result()
                    all_sentences.extend(sentences)
                    self.relation_counts[relation] = len(sentences)
                    print(f"ğŸ¯ {relation} å…³ç³»å®Œæˆï¼Œç”Ÿæˆ {len(sentences)} æ¡å¥å­")
                except Exception as e:
                    logger.error(f"ç”Ÿæˆ {relation} å…³ç³»æ•°æ®å¤±è´¥: {e}")
                    self.relation_counts[relation] = 0
        
        self.generated_sentences = all_sentences
        
        # æœ€ç»ˆä¿å­˜å®æ—¶ç¼“å­˜
        self.save_realtime_cache(force=True)
        
        print(f"\nâœ… é«˜æ•ˆæ•°æ®ç”Ÿæˆå®Œæˆ! æ€»è®¡: {len(all_sentences)} æ¡")
        return all_sentences
    
    def generate_all_data(self):
        """ç”Ÿæˆæ‰€æœ‰å…³ç³»çš„æ•°æ®"""
        print(f"\nğŸš€ å¼€å§‹ç”ŸæˆçŸ¥è¯†å›¾è°±æ•°æ®")
        print(f"ç›®æ ‡: æ¯ä¸ªå…³ç³» {self.records_per_relation} æ¡ï¼Œå…± {len(RELATION_TYPES)} ä¸ªå…³ç³»")
        
        all_sentences = []
        
        for relation in RELATION_TYPES.keys():
            sentences = self.generate_sentences_for_relation(relation, self.records_per_relation)
            all_sentences.extend(sentences)
            self.relation_counts[relation] = len(sentences)
        
        self.generated_sentences = all_sentences
        
        print(f"\nâœ… æ•°æ®ç”Ÿæˆå®Œæˆ!")
        print(f"æ€»è®¡ç”Ÿæˆ: {len(all_sentences)} æ¡å¥å­")
        
        # æ˜¾ç¤ºå„å…³ç³»ç»Ÿè®¡
        for relation, count in self.relation_counts.items():
            percentage = (count / len(all_sentences)) * 100 if all_sentences else 0
            print(f"  - {relation}: {count} æ¡ ({percentage:.1f}%)")
    
    def save_data(self):
        """ä¿å­˜ç”Ÿæˆçš„æ•°æ®"""
        print(f"\nğŸ’¾ ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶...")
        
        # å‡†å¤‡JSONæ•°æ®ç»“æ„
        json_data = {
            "sentences": self.generated_sentences,
            "statistics": {
                "total_sentences": len(self.generated_sentences),
                "relation_counts": dict(self.relation_counts),
                "generation_time": time.strftime("%Y-%m-%d %H:%M:%S")
            }
        }
        
        # ä¿å­˜ä¸ºJSONæ ¼å¼
        json_file = self.output_file.replace('.txt', '.json')
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ä¸ºæ–‡æœ¬æ ¼å¼
        with open(self.output_file, 'w', encoding='utf-8') as f:
            for item in self.generated_sentences:
                f.write(f"{item['sentence']}\t{item['relation']}\t{item['entity1']}\t{item['entity2']}\n")
        
        print(f"âœ… æ•°æ®å·²ä¿å­˜:")
        print(f"  - JSONæ ¼å¼: {json_file}")
        print(f"  - æ–‡æœ¬æ ¼å¼: {self.output_file}")
    
    def export_to_deepke_format(self):
        """å¯¼å‡ºä¸ºDeepKEè®­ç»ƒæ ¼å¼"""
        print(f"\nğŸ“¤ å¯¼å‡ºDeepKEè®­ç»ƒæ•°æ®...")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = Config.ANNOTATION_OUTPUT_DIR
        os.makedirs(output_dir, exist_ok=True)
        
        # éšæœºæ‰“ä¹±æ•°æ®
        random.shuffle(self.generated_sentences)
        
        # åˆ†å‰²æ•°æ®é›† (70% è®­ç»ƒ, 15% éªŒè¯, 15% æµ‹è¯•)
        total = len(self.generated_sentences)
        train_size = int(total * 0.7)
        valid_size = int(total * 0.15)
        
        train_data = self.generated_sentences[:train_size]
        valid_data = self.generated_sentences[train_size:train_size + valid_size]
        test_data = self.generated_sentences[train_size + valid_size:]
        
        # ä¿å­˜è®­ç»ƒé›†
        train_file = os.path.join(output_dir, Config.TRAIN_FILE)
        self.save_csv_data(train_data, train_file)
        
        # ä¿å­˜éªŒè¯é›†
        valid_file = os.path.join(output_dir, Config.VALID_FILE)
        self.save_csv_data(valid_data, valid_file)
        
        # ä¿å­˜æµ‹è¯•é›†
        test_file = os.path.join(output_dir, Config.TEST_FILE)
        self.save_csv_data(test_data, test_file)
        
        print(f"âœ… DeepKEæ•°æ®å¯¼å‡ºå®Œæˆ:")
        print(f"  - è®­ç»ƒé›†: {train_file} ({len(train_data)} æ¡)")
        print(f"  - éªŒè¯é›†: {valid_file} ({len(valid_data)} æ¡)")
        print(f"  - æµ‹è¯•é›†: {test_file} ({len(test_data)} æ¡)")
    
    def save_csv_data(self, data, filename):
        """ä¿å­˜CSVæ ¼å¼æ•°æ®"""
        with open(filename, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['sentence', 'relation', 'head', 'head_offset', 'tail', 'tail_offset'])
            
            for item in data:
                sentence = item['sentence']
                head = item['head']
                tail = item['tail']
                relation = item['relation']
                
                # è®¡ç®—å®ä½“åœ¨å¥å­ä¸­çš„ä½ç½®
                head_offset = sentence.find(head)
                tail_offset = sentence.find(tail)
                
                # å¦‚æœæ‰¾ä¸åˆ°å®ä½“ä½ç½®ï¼Œè·³è¿‡è¿™æ¡æ•°æ®
                if head_offset == -1 or tail_offset == -1:
                    continue
                
                writer.writerow([
                    sentence, relation, head, 
                    f"{head_offset},{head_offset + len(head)}", 
                    tail, 
                    f"{tail_offset},{tail_offset + len(tail)}"
                ])
    
    def run(self, fast_mode=True):
        """è¿è¡Œæ•°æ®ç”Ÿæˆæµç¨‹"""
        try:
            if fast_mode:
                print("\nâš¡ ä½¿ç”¨é«˜æ•ˆæ¨¡å¼ç”Ÿæˆæ•°æ®...")
                self.generate_all_data_fast()
            else:
                print("\nğŸŒ ä½¿ç”¨æ ‡å‡†æ¨¡å¼ç”Ÿæˆæ•°æ®...")
                self.generate_all_data()
            
            # ä¿å­˜æ•°æ®
            self.save_data()
            
            # å¯¼å‡ºDeepKEæ ¼å¼
            self.export_to_deepke_format()
            
            # ä¿å­˜ç¼“å­˜
            if self.enable_cache:
                self.save_cache()
            
            print(f"\nğŸ‰ æ•°æ®ç”Ÿæˆæµç¨‹å®Œæˆ!")
            print(f"ğŸ“Š ç”Ÿæˆç»Ÿè®¡:")
            for relation, count in self.relation_counts.items():
                print(f"   - {relation}: {count} æ¡")
            
        except Exception as e:
            logger.error(f"æ•°æ®ç”Ÿæˆå¤±è´¥: {e}")
            # å³ä½¿å¤±è´¥ä¹Ÿè¦ä¿å­˜ç¼“å­˜
            if self.enable_cache:
                self.save_cache()
            raise

# ========================= ä¸»å‡½æ•° =============================
def main():
    """ä¸»å‡½æ•°"""
    try:
        # æ£€æŸ¥APIå¯†é’¥
        api_key = os.getenv("ARK_API_KEY")
        if not api_key:
            print("âŒ é”™è¯¯: è¯·è®¾ç½®ç¯å¢ƒå˜é‡ ARK_API_KEY")
            return
        
        print("ğŸš€ å¯åŠ¨çŸ¥è¯†å›¾è°±æ•°æ®ç”Ÿæˆå™¨...")
        
        # åˆå§‹åŒ–ç”Ÿæˆå™¨ï¼ˆä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        generator = KnowledgeGraphGenerator(api_key)
        
        # è¿è¡Œæ•°æ®ç”Ÿæˆï¼ˆé»˜è®¤ä½¿ç”¨å¿«é€Ÿæ¨¡å¼ï¼‰
        generator.run(fast_mode=True)
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()