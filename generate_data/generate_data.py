#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®ç»“æ„è¯¾ç¨‹çŸ¥è¯†å›¾è°±æ•°æ®ç”Ÿæˆå™¨ - å…³ç³»å‡ç­‰åˆ†å¸ƒç‰ˆ
ç”ŸæˆåŒ…å«8ç§å…³ç³»ç±»å‹çš„é«˜è´¨é‡çŸ¥è¯†å›¾è°±æ„å»ºè®­ç»ƒæ•°æ®
å…³ç³»ç±»å‹: rely, b-rely, belg, b-belg, syno, anto, attr, b-attr
"""

import os
import time
import concurrent.futures
import openai
from tqdm import tqdm
import random
import logging
import re
import json
import csv
from typing import List, Dict, Tuple, Set
from collections import defaultdict, Counter

# ç¦ç”¨HTTPè¯·æ±‚æ—¥å¿—
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)
logging.getLogger("openai").setLevel(logging.WARNING)

# ============================= ç»Ÿä¸€é…ç½® =============================
class Config:
    """ç»Ÿä¸€é…ç½®ç±»"""
    # APIé…ç½®
    API_KEY = os.environ.get("ARK_API_KEY")
    BASE_URL = "https://ark.cn-beijing.volces.com/api/v3"
    MODEL = "doubao-1-5-lite-32k-250115"
    TIMEOUT = 30
    RETRY_COUNT = 2
    DELAY_BETWEEN_REQUESTS = 0
    CONCURRENCY = 30
    
    # æ•°æ®ç”Ÿæˆé…ç½®
    NUM_RECORDS = 30000
    MAX_PROMPTS = 1000
    MIN_PROMPTS_PER_RELATION = 5
    
    # æ–‡ä»¶è·¯å¾„é…ç½®
    OUTPUT_FILE = "/root/KG_inde/generate_data/data_backups/knowledge_graph_sentences_2.txt"
    PROMPTS_FILE = "kg_prompts.txt"
    VOCAB_DICT_FILE = "../DeepKE/example/ner/prepare-data/vocab_dict.csv"
    
    # æ ‡æ³¨æ–‡ä»¶è¾“å‡ºè·¯å¾„
    ANNOTATION_OUTPUT_DIR = "/root/KG_inde/DeepKE/example/re/standard/data/origin"
    RELATION_FILE = "relation.csv"
    TRAIN_FILE = "train.csv"
    TEST_FILE = "test.csv"
    VALID_FILE = "valid.csv"

# ========================= æ‰©å±•çŸ¥è¯†åº“ =============================
KNOWLEDGE_GRAPH_BASE = {
    # åŸºç¡€æ•°æ®ç»“æ„
    "åŸºç¡€æ•°æ®ç»“æ„": [
        "æ•°ç»„", "é“¾è¡¨", "æ ˆ", "é˜Ÿåˆ—", "å“ˆå¸Œè¡¨", "é›†åˆ", "æ˜ å°„", "å­—ç¬¦ä¸²",
        "å•é“¾è¡¨", "åŒå‘é“¾è¡¨", "å¾ªç¯é“¾è¡¨", "é™æ€æ•°ç»„", "åŠ¨æ€æ•°ç»„", "å¾ªç¯é˜Ÿåˆ—", 
        "åŒç«¯é˜Ÿåˆ—", "ä¼˜å…ˆé˜Ÿåˆ—", "æ•£åˆ—è¡¨", "å¼€æ”¾å¯»å€", "é“¾åœ°å€æ³•", "åŒå‘é˜Ÿåˆ—",
        "çº¿æ€§è¡¨", "çº¿æ€§ç»“æ„", "éçº¿æ€§ç»“æ„", "æŠ½è±¡æ•°æ®ç±»å‹", "æ•°æ®å…ƒç´ ", 
        "æ•°æ®ç±»å‹", "æ•°æ®é¡¹", "é€»è¾‘ç»“æ„", "ç‰©ç†ç»“æ„", "å­˜å‚¨ç»“æ„", "æœ‰åºè¡¨",
        "æ— åºè¡¨", "æœ‰åºåºåˆ—", "æ— åºåºåˆ—", "è¡¨å¤´", "è¡¨å°¾", "è¡¨é•¿", "ç©ºè¡¨",
        "é“¾è¡¨èŠ‚ç‚¹", "å¤´èŠ‚ç‚¹", "å°¾èŠ‚ç‚¹", "å‰é©±èŠ‚ç‚¹", "åç»§èŠ‚ç‚¹", "æŒ‡é’ˆ", "å¼•ç”¨",
        "å“ˆå¸Œå‡½æ•°", "å†²çªè§£å†³", "æ•£åˆ—æ–‡ä»¶"
    ],
    
    # æ ‘å½¢ç»“æ„
    "æ ‘å½¢ç»“æ„": [
        "æ ‘", "äºŒå‰æ ‘", "äºŒå‰æœç´¢æ ‘", "AVLæ ‘", "çº¢é»‘æ ‘", "Bæ ‘", "B+æ ‘",
        "å®Œå…¨äºŒå‰æ ‘", "æ»¡äºŒå‰æ ‘", "å¹³è¡¡äºŒå‰æ ‘", "å­—å…¸æ ‘", "å‰ç¼€æ ‘", "åç¼€æ ‘",
        "çº¿æ®µæ ‘", "æ ‘çŠ¶æ•°ç»„", "å †", "å¤§æ ¹å †", "å°æ ¹å †", "äºŒé¡¹å †", "æ–æ³¢é‚£å¥‘å †",
        "æ ¹èŠ‚ç‚¹", "å¶å­èŠ‚ç‚¹", "å†…éƒ¨èŠ‚ç‚¹", "å·¦å­æ ‘", "å³å­æ ‘", "å­æ ‘", "çˆ¶èŠ‚ç‚¹", "å­èŠ‚ç‚¹",
        "æ ‘çš„é«˜åº¦", "æ ‘çš„æ·±åº¦", "æ ‘çš„åº¦", "æ ‘çš„èŠ‚ç‚¹", "å¹³è¡¡å› å­"
    ],
    
    # å›¾ç»“æ„
    "å›¾ç»“æ„": [
        "å›¾", "æœ‰å‘å›¾", "æ— å‘å›¾", "åŠ æƒå›¾", "è¿é€šå›¾", "å¼ºè¿é€šå›¾", "å¼±è¿é€šå›¾",
        "ç¨€ç–å›¾", "ç¨ å¯†å›¾", "é‚»æ¥çŸ©é˜µ", "é‚»æ¥è¡¨", "è¾¹è¡¨", "åå­—é“¾è¡¨", "é‚»æ¥å¤šé‡è¡¨",
        "é¡¶ç‚¹", "è¾¹", "è·¯å¾„", "å›è·¯", "ç¯", "åº¦", "å…¥åº¦", "å‡ºåº¦", "è¿é€šåˆ†é‡",
        "å¼ºè¿é€šåˆ†é‡", "æœ€å°ç”Ÿæˆæ ‘", "ç”Ÿæˆæ£®æ—", "å›¾çš„é¡¶ç‚¹", "å›¾çš„è¾¹", "é¡¶ç‚¹åº¦æ•°",
        "è¾¹çš„æƒé‡", "è¾¹çš„æ–¹å‘", "éè¿é€šå›¾", "æœ€é•¿è·¯å¾„"
    ],
    
    # æ’åºç®—æ³•
    "æ’åºç®—æ³•": [
        "å†’æ³¡æ’åº", "é€‰æ‹©æ’åº", "æ’å…¥æ’åº", "å¿«é€Ÿæ’åº", "å½’å¹¶æ’åº", "å †æ’åº",
        "è®¡æ•°æ’åº", "åŸºæ•°æ’åº", "æ¡¶æ’åº", "å¸Œå°”æ’åº", "ç›´æ¥æ’å…¥æ’åº",
        "äºŒè·¯å½’å¹¶", "å¤šè·¯å½’å¹¶", "å¤–éƒ¨æ’åº", "å†…éƒ¨æ’åº", "ç¨³å®šæ’åº", "ä¸ç¨³å®šæ’åº",
        "æ¯”è¾ƒæ’åº", "éæ¯”è¾ƒæ’åº", "åŸåœ°æ’åº", "äº¤æ¢æ’åº", "ç®€å•æ’åº", "æ’åºç®—æ³•ç¨³å®šæ€§",
        "ç¨³å®šæ€§"
    ],
    
    # æŸ¥æ‰¾ç®—æ³•
    "æŸ¥æ‰¾ç®—æ³•": [
        "çº¿æ€§æŸ¥æ‰¾", "äºŒåˆ†æŸ¥æ‰¾", "æ’å€¼æŸ¥æ‰¾", "æŒ‡æ•°æŸ¥æ‰¾", "å“ˆå¸ŒæŸ¥æ‰¾",
        "é¡ºåºæŸ¥æ‰¾", "æŠ˜åŠæŸ¥æ‰¾", "åˆ†å—æŸ¥æ‰¾", "æ ‘è¡¨æŸ¥æ‰¾", "åŠ¨æ€æŸ¥æ‰¾",
        "é™æ€æŸ¥æ‰¾", "æŸ¥æ‰¾æˆåŠŸ", "æŸ¥æ‰¾å¤±è´¥", "å¹³å‡æŸ¥æ‰¾é•¿åº¦"
    ],
    
    # å›¾ç®—æ³•
    "å›¾ç®—æ³•": [
        "æ·±åº¦ä¼˜å…ˆæœç´¢", "å¹¿åº¦ä¼˜å…ˆæœç´¢", "Dijkstraç®—æ³•", "Floydç®—æ³•", "Bellman-Fordç®—æ³•",
        "Kruskalç®—æ³•", "Primç®—æ³•", "æ‹“æ‰‘æ’åº", "å…³é”®è·¯å¾„", "æœ€çŸ­è·¯å¾„",
        "å•æºæœ€çŸ­è·¯å¾„", "å¤šæºæœ€çŸ­è·¯å¾„", "è´Ÿæƒè¾¹", "è´Ÿæƒå›è·¯", "AOVç½‘", "AOEç½‘",
        "è¿ªæ°æ–¯ç‰¹æ‹‰ç®—æ³•", "å¼—æ´›ä¼Šå¾·ç®—æ³•", "å…‹é²æ–¯å¡å°”ç®—æ³•", "æ™®é‡Œå§†ç®—æ³•", "AOV ç½‘", "AOE ç½‘"
    ],
    
    # åŠ¨æ€è§„åˆ’ä¸è´ªå¿ƒ
    "ç®—æ³•è®¾è®¡": [
        "åŠ¨æ€è§„åˆ’", "è´ªå¿ƒç®—æ³•", "åˆ†æ²»ç®—æ³•", "å›æº¯ç®—æ³•", "åˆ†æ”¯é™ç•Œ",
        "é€’å½’", "è¿­ä»£", "è®°å¿†åŒ–æœç´¢", "çŠ¶æ€è½¬ç§»", "æœ€ä¼˜å­ç»“æ„",
        "é‡å å­é—®é¢˜", "è´ªå¿ƒé€‰æ‹©æ€§è´¨", "å±€éƒ¨æœ€ä¼˜", "å…¨å±€æœ€ä¼˜", "åˆ†æ²»",
        "è´ªå¿ƒ", "è´ªå¿ƒç­–ç•¥", "å›æº¯æ³•", "åŠ¨æ€è§„åˆ’å…¥é—¨", "é€’å½’åŸºç¡€", "é€’æ¨",
        "è¿­ä»£æ³•", "ç©·ä¸¾æ³•", "ç®—æ³•æ­¥éª¤", "ç®—æ³•çš„å¯è¡Œæ€§", "ç®—æ³•çš„æœ‰ç©·æ€§", "ç®—æ³•çš„ç¡®å®šæ€§"
    ],
    
    # å¤æ‚åº¦åˆ†æ
    "å¤æ‚åº¦åˆ†æ": [
        "æ—¶é—´å¤æ‚åº¦", "ç©ºé—´å¤æ‚åº¦", "æœ€å¥½æƒ…å†µ", "æœ€åæƒ…å†µ", "å¹³å‡æƒ…å†µ",
        "æ¸è¿‘å¤æ‚åº¦", "å¤§Oè®°å·", "Î˜è®°å·", "Î©è®°å·", "é€’å½’å¤æ‚åº¦",
        "æ‘Šè¿˜åˆ†æ", "åŠ¿èƒ½æ–¹æ³•", "èšåˆåˆ†æ", "ä¼šè®¡æ–¹æ³•", "æ“ä½œæ•ˆç‡"
    ],
    
    # æ•°æ®ç»“æ„æ“ä½œ
    "åŸºæœ¬æ“ä½œ": [
        "æ’å…¥", "åˆ é™¤", "æŸ¥æ‰¾", "éå†", "æ’åº", "åˆå¹¶", "åˆ†å‰²", "æ—‹è½¬",
        "å¹³è¡¡", "æ‰©å®¹", "ç¼©å®¹", "åˆå§‹åŒ–", "é”€æ¯", "å¤åˆ¶", "ç§»åŠ¨",
        "å…¥æ ˆ", "å‡ºæ ˆ", "å…¥é˜Ÿ", "å‡ºé˜Ÿ", "å‰åºéå†", "ä¸­åºéå†", "ååºéå†", "å±‚åºéå†",
        "è®¿é—®", "æ›´æ–°", "ä½åº"
    ],

    # é«˜çº§æ•°æ®ç»“æ„
    "é«˜çº§æ•°æ®ç»“æ„": [
        "å¹¶æŸ¥é›†", "è·³è·ƒè¡¨", "å¸ƒéš†è¿‡æ»¤å™¨", "LRUç¼“å­˜", "LFUç¼“å­˜", "å­—å…¸æ ‘",
        "åç¼€æ•°ç»„", "KMPç®—æ³•", "ACè‡ªåŠ¨æœº", "å¯æŒä¹…åŒ–æ•°æ®ç»“æ„", "å‡½æ•°å¼æ•°æ®ç»“æ„",
        "ä¸ç›¸äº¤é›†åˆ", "è·¯å¾„å‹ç¼©", "æŒ‰ç§©åˆå¹¶"
    ],
    
    # å­˜å‚¨ç»“æ„
    "å­˜å‚¨ç»“æ„": [
        "é¡ºåºå­˜å‚¨", "é“¾å¼å­˜å‚¨", "ç´¢å¼•å­˜å‚¨", "æ•£åˆ—å­˜å‚¨", "éšæœºè®¿é—®", "é¡ºåºè®¿é—®",
        "å†…å­˜åˆ†é…", "å†…å­˜å›æ”¶", "å†…å­˜æ± ", "å¯¹è±¡æ± ", "å¼•ç”¨è®¡æ•°", "åƒåœ¾å›æ”¶",
        "æ ˆå†…å­˜", "å †å†…å­˜", "é™æ€å†…å­˜", "åŠ¨æ€å†…å­˜"
    ]
}

# ========================= å…³ç³»å®šä¹‰ =============================
RELATION_TYPES = {
    "rely": "ä¾èµ–å…³ç³»",      # Aä¾èµ–B
    "b-rely": "è¢«ä¾èµ–å…³ç³»",  # Aè¢«Bä¾èµ–  
    "belg": "æ‰€å±å…³ç³»",      # Aå±äºB
    "b-belg": "è¢«æ‰€å±å…³ç³»",  # AåŒ…å«B
    "syno": "åŒä¹‰å…³ç³»",      # Aä¸BåŒä¹‰
    "anto": "ç›¸å¯¹å…³ç³»",  # Aä¸Bç›¸å¯¹
    "attr": "å±æ€§å…³ç³»",      # Aæ˜¯Bçš„å±æ€§
    "b-attr": "è¢«å±æ€§å…³ç³»"   # Aå…·æœ‰å±æ€§B
}

# ========================= å…³ç³»æ¨¡æ¿ =============================
RELATION_TEMPLATES = {
    "rely": [
        "{entity1}çš„å®ç°éœ€è¦ä¾èµ–{entity2}",
        "{entity1}ç®—æ³•ä¾èµ–äº{entity2}çš„æ”¯æŒ",
        "{entity1}çš„æ‰§è¡Œä¾èµ–{entity2}æä¾›çš„åŠŸèƒ½",
        "{entity1}æ“ä½œéœ€è¦{entity2}ä½œä¸ºåŸºç¡€",
        "{entity1}çš„æ€§èƒ½ä¾èµ–äº{entity2}çš„æ•ˆç‡"
    ],
    "b-rely": [
        "{entity1}è¢«{entity2}ç®—æ³•æ‰€ä¾èµ–",
        "{entity1}ä¸º{entity2}æä¾›åŸºç¡€æ”¯æŒ",
        "{entity1}æ˜¯{entity2}å®ç°çš„å‰ææ¡ä»¶",
        "{entity1}æ”¯æ’‘ç€{entity2}çš„è¿è¡Œ",
        "{entity1}æ˜¯{entity2}ä¸å¯ç¼ºå°‘çš„ç»„æˆéƒ¨åˆ†"
    ],
    "belg": [
        "{entity1}å±äº{entity2}çš„èŒƒç•´",
        "{entity1}æ˜¯{entity2}çš„ä¸€ç§ç±»å‹",
        "{entity1}å½’ç±»ä¸º{entity2}",
        "{entity1}æ˜¯{entity2}ä¸­çš„ä¸€å‘˜",
        "{entity1}è¢«åˆ’åˆ†åˆ°{entity2}ç±»åˆ«ä¸­"
    ],
    "b-belg": [
        "{entity1}åŒ…å«{entity2}è¿™ç§ç±»å‹",
        "{entity1}æ¶µç›–äº†{entity2}",
        "{entity1}çš„èŒƒå›´åŒ…æ‹¬{entity2}",
        "{entity1}å›Šæ‹¬{entity2}åœ¨å†…",
        "{entity1}æ˜¯{entity2}çš„ä¸Šçº§åˆ†ç±»"
    ],
    "syno": [
        "{entity1}ä¸{entity2}æ˜¯åŒä¹‰æ¦‚å¿µ",
        "{entity1}å’Œ{entity2}è¡¨ç¤ºç›¸åŒå«ä¹‰",
        "{entity1}ç­‰åŒäº{entity2}",
        "{entity1}å°±æ˜¯{entity2}çš„å¦ä¸€ç§è¯´æ³•",
        "{entity1}ä¸{entity2}åœ¨æœ¬è´¨ä¸Šç›¸åŒ"
    ],
    "anto": [
        "{entity1}ä¸{entity2}å½¢æˆå¯¹æ¯”å…³ç³»",
        "{entity1}å’Œ{entity2}æ˜¯ç›¸å¯¹çš„æ¦‚å¿µ",
        "{entity1}ä¸{entity2}äº’ä¸ºå¯¹ç«‹",
        "{entity1}å’Œ{entity2}å‘ˆç°ç›¸åç‰¹æ€§",
        "{entity1}ä¸{entity2}æ„æˆå¯¹å¶å…³ç³»"
    ],
    "attr": [
        "{entity1}æ˜¯{entity2}çš„é‡è¦å±æ€§",
        "{entity1}è¡¨å¾äº†{entity2}çš„ç‰¹æ€§",
        "{entity1}æè¿°{entity2}çš„æ€§è´¨",
        "{entity1}æ˜¯è¡¡é‡{entity2}çš„æŒ‡æ ‡",
        "{entity1}åæ˜ äº†{entity2}çš„ç‰¹å¾"
    ],
    "b-attr": [
        "{entity1}å…·æœ‰{entity2}è¿™ä¸€å±æ€§",
        "{entity1}çš„ç‰¹å¾åŒ…æ‹¬{entity2}",
        "{entity1}è¡¨ç°å‡º{entity2}çš„æ€§è´¨",
        "{entity1}æ‹¥æœ‰{entity2}ç‰¹æ€§",
        "{entity1}å±•ç°äº†{entity2}çš„ç‰¹ç‚¹"
    ]
}

# ========================= æ ¸å¿ƒå‡½æ•° =============================
def create_client():
    """åˆ›å»ºOpenAIå®¢æˆ·ç«¯"""
    openai.api_key = Config.API_KEY
    openai.api_base = Config.BASE_URL
    return openai

def get_all_entities():
    """è·å–æ‰€æœ‰å®ä½“åˆ—è¡¨"""
    all_entities = []
    for category, entities in KNOWLEDGE_GRAPH_BASE.items():
        all_entities.extend(entities)
    return list(set(all_entities))  # å»é‡

def generate_relation_prompts(num_records):
    """ç”Ÿæˆ8ç§å…³ç³»ç±»å‹å‡ç­‰åˆ†å¸ƒçš„æç¤ºè¯"""
    prompts = []
    all_entities = get_all_entities()
    
    # æ¯ç§å…³ç³»ç±»å‹åˆ†é…ç›¸ç­‰æ•°é‡
    records_per_relation = num_records // len(RELATION_TYPES)
    remaining_records = num_records % len(RELATION_TYPES)
    
    relation_counts = {}
    for relation in RELATION_TYPES.keys():
        count = records_per_relation
        if remaining_records > 0:
            count += 1
            remaining_records -= 1
        relation_counts[relation] = count
    
    print(f"ğŸ“Š å…³ç³»åˆ†å¸ƒè®¡åˆ’: {relation_counts}")
    
    # ä¸ºæ¯ç§å…³ç³»ç”Ÿæˆæç¤ºè¯
    for relation_type, count in relation_counts.items():
        templates = RELATION_TEMPLATES[relation_type]
        
        for _ in range(count):
            # éšæœºé€‰æ‹©ä¸¤ä¸ªä¸åŒçš„å®ä½“
            entity1, entity2 = random.sample(all_entities, 2)
            template = random.choice(templates)
            
            # ç”Ÿæˆæç¤ºè¯
            prompt = template.format(entity1=entity1, entity2=entity2)
            prompts.append((prompt, relation_type, entity1, entity2))
    
    random.shuffle(prompts)
    return prompts

def is_valid_kg_response(text, entities):
    """éªŒè¯å“åº”æ˜¯å¦é€‚åˆçŸ¥è¯†å›¾è°±æ„å»º"""
    if not text or len(text.strip()) < 15:
        return False
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›¸å…³å®ä½“
    has_entity = any(entity in text for entity in entities)
    
    # æ£€æŸ¥æ— æ•ˆæ¨¡å¼
    invalid_patterns = [
        r'æˆ‘æ— æ³•|æˆ‘ä¸èƒ½|æŠ±æ­‰|å¯¹ä¸èµ·',
        r'ä½œä¸ºAI|ä½œä¸ºè¯­è¨€æ¨¡å‹',
        r'è¯·æ³¨æ„|éœ€è¦æ³¨æ„çš„æ˜¯',
        r'^\s*$'
    ]
    
    for pattern in invalid_patterns:
        if re.search(pattern, text):
            return False
    
    return has_entity and 15 <= len(text) <= 200

def call_api_with_retry(prompt_data):
    """å¸¦é‡è¯•æœºåˆ¶çš„APIè°ƒç”¨"""
    prompt, relation_type, entity1, entity2 = prompt_data
    client = create_client()
    
    for attempt in range(Config.RETRY_COUNT):
        try:
            if Config.DELAY_BETWEEN_REQUESTS > 0:
                time.sleep(Config.DELAY_BETWEEN_REQUESTS)
            
            response = openai.ChatCompletion.create(
                model=Config.MODEL,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯æ•°æ®ç»“æ„ä¸“å®¶ï¼Œè¯·ç”Ÿæˆå‡†ç¡®ç®€æ´çš„æŠ€æœ¯æè¿°ï¼Œç¡®ä¿åŒ…å«æŒ‡å®šçš„å®ä½“æ¦‚å¿µã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=120,
                temperature=0.7,
                timeout=Config.TIMEOUT
            )
            
            content = response.choices[0].message.content.strip()
            
            if is_valid_kg_response(content, [entity1, entity2]):
                return {
                    'text': content,
                    'relation': relation_type,
                    'entity1': entity1,
                    'entity2': entity2
                }
            else:
                continue
                
        except Exception as e:
            if attempt == Config.RETRY_COUNT - 1:
                logging.warning(f"APIè°ƒç”¨æœ€ç»ˆå¤±è´¥: {e}")
                return None
            logging.debug(f"APIè°ƒç”¨é‡è¯• {attempt + 1}/{Config.RETRY_COUNT}: {e}")
            time.sleep(0.5)
    
    return None

def post_process_sentences(results):
    """æ•°æ®åå¤„ç†ä¼˜åŒ–"""
    print("\nğŸ”§ æ­£åœ¨è¿›è¡Œæ•°æ®åå¤„ç†ä¼˜åŒ–...")
    
    processed = []
    relation_stats = Counter()
    
    for result in results:
        if not result:
            continue
            
        text = result['text']
        relation = result['relation']
        
        # æ¸…ç†æ–‡æœ¬
        cleaned = re.sub(r'\s+', ' ', text.strip())
        cleaned = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹]+$', '', cleaned)
        
        # ç¡®ä¿ä»¥å¥å·ç»“å°¾
        if not cleaned.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ')):
            cleaned += 'ã€‚'
        
        # é•¿åº¦æ£€æŸ¥
        if 15 <= len(cleaned) <= 200:
            processed.append({
                'text': cleaned,
                'relation': relation,
                'entity1': result['entity1'],
                'entity2': result['entity2']
            })
            relation_stats[relation] += 1
    
    print(f"âœ… åå¤„ç†å®Œæˆ: ä¿ç•™ {len(processed)} æ¡")
    print(f"ğŸ“Š å…³ç³»åˆ†å¸ƒç»Ÿè®¡: {dict(relation_stats)}")
    return processed

def process_large_batch(prompt_data_list):
    """æ‰¹é‡å¤„ç†æç¤ºè¯"""
    print(f"\nğŸš€ å¼€å§‹æ‰¹é‡ç”Ÿæˆ {len(prompt_data_list)} æ¡æ•°æ®...")
    
    results = []
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=Config.CONCURRENCY) as executor:
        future_to_prompt = {executor.submit(call_api_with_retry, prompt_data): prompt_data for prompt_data in prompt_data_list}
        
        with tqdm(total=len(prompt_data_list), desc="ç”Ÿæˆæ•°æ®", unit="æ¡") as pbar:
            for future in concurrent.futures.as_completed(future_to_prompt):
                result = future.result()
                if result:
                    results.append(result)
                pbar.update(1)
    
    return results

def save_data_with_relations(results, filename):
    """ä¿å­˜å¸¦å…³ç³»æ ‡æ³¨çš„æ•°æ®"""
    try:
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        
        # ä¿å­˜åŸå§‹æ ¼å¼
        with open(filename, 'w', encoding='utf-8') as f:
            for result in results:
                f.write(result['text'] + '\n')
        
        # ä¿å­˜å¸¦å…³ç³»æ ‡æ³¨çš„æ ¼å¼
        relation_filename = filename.replace('.txt', '_with_relations.jsonl')
        with open(relation_filename, 'w', encoding='utf-8') as f:
            for result in results:
                json_line = json.dumps({
                    'text': result['text'],
                    'relation': result['relation'],
                    'entity1': result['entity1'],
                    'entity2': result['entity2']
                }, ensure_ascii=False)
                f.write(json_line + '\n')
        
        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
        print(f"ğŸ’¾ å…³ç³»æ•°æ®å·²ä¿å­˜åˆ°: {relation_filename}")
        
        return analyze_data_quality(results)
        
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
        return None

def analyze_data_quality(results):
    """åˆ†æç”Ÿæˆæ•°æ®çš„è´¨é‡"""
    print("\nğŸ“Š æ•°æ®è´¨é‡åˆ†æ:")
    
    all_entities = get_all_entities()
    
    # ç»Ÿè®¡å…³ç³»åˆ†å¸ƒ
    relation_counts = Counter(result['relation'] for result in results)
    print(f"ğŸ”— å…³ç³»åˆ†å¸ƒ: {dict(relation_counts)}")
    
    # ç»Ÿè®¡å®ä½“è¦†ç›–ç‡
    used_entities = set()
    for result in results:
        used_entities.add(result['entity1'])
        used_entities.add(result['entity2'])
    
    coverage_rate = (len(used_entities) / len(all_entities)) * 100
    print(f"ğŸ¯ å®ä½“è¦†ç›–ç‡: {len(used_entities)}/{len(all_entities)} ({coverage_rate:.1f}%)")
    
    # ç»Ÿè®¡å¥å­é•¿åº¦
    lengths = [len(result['text']) for result in results]
    avg_length = sum(lengths) / len(lengths)
    print(f"ğŸ“ å¹³å‡å¥å­é•¿åº¦: {avg_length:.1f}å­—")
    
    return {
        'relation_distribution': dict(relation_counts),
        'entity_coverage': coverage_rate,
        'used_entities': len(used_entities),
        'total_entities': len(all_entities),
        'avg_length': avg_length
    }

def update_vocab_dict():
    """æ›´æ–°è¯å…¸æ–‡ä»¶"""
    vocab_file = "/root/KG_inde/DeepKE/example/ner/prepare-data/vocab_dict.csv"
    
    # è¯»å–ç°æœ‰è¯å…¸
    existing_entities = set()
    if os.path.exists(vocab_file):
        with open(vocab_file, 'r', encoding='utf-8') as f:
            for line in f:
                if ',' in line:
                    entity = line.strip().split(',')[0]
                    existing_entities.add(entity)
    
    # è·å–æ‰€æœ‰æ–°å®ä½“
    all_entities = get_all_entities()
    new_entities = []
    
    for entity in all_entities:
        if entity not in existing_entities:
            # æ ¹æ®å®ä½“ç±»å‹åˆ†é…æ ‡ç­¾
            if any(entity in KNOWLEDGE_GRAPH_BASE[cat] for cat in ["æ’åºç®—æ³•", "æŸ¥æ‰¾ç®—æ³•", "å›¾ç®—æ³•", "ç®—æ³•è®¾è®¡"]):
                label = "ARI"  # ç®—æ³•
            else:
                label = "CON"  # æ¦‚å¿µ
            new_entities.append(f"{entity},{label}")
    
    # è¿½åŠ æ–°å®ä½“åˆ°æ–‡ä»¶
    if new_entities:
        with open(vocab_file, 'a', encoding='utf-8') as f:
            for entity_line in new_entities:
                f.write(entity_line + '\n')
        print(f"ğŸ“ å·²å‘è¯å…¸æ·»åŠ  {len(new_entities)} ä¸ªæ–°å®ä½“")
    else:
        print("ğŸ“ è¯å…¸å·²åŒ…å«æ‰€æœ‰å®ä½“ï¼Œæ— éœ€æ›´æ–°")

# DataStructureKGConfigç±»å·²åˆå¹¶åˆ°Configç±»ä¸­

def generate_sentence_for_relation(relation: str, head_entity: str, tail_entity: str) -> str:
    """
    æ ¹æ®å…³ç³»ç±»å‹å’Œå®ä½“ç”Ÿæˆå¥å­
    
    Args:
        relation: å…³ç³»ç±»å‹
        head_entity: å¤´å®ä½“
        tail_entity: å°¾å®ä½“
    
    Returns:
        ç”Ÿæˆçš„å¥å­
    """
    templates = {
        'syno': [
            f"{head_entity}å’Œ{tail_entity}æ˜¯åŒä¹‰æ¦‚å¿µ",
            f"{head_entity}ä¸{tail_entity}å…·æœ‰ç›¸åŒçš„å«ä¹‰",
            f"{head_entity}ç­‰åŒäº{tail_entity}",
            f"åœ¨æ•°æ®ç»“æ„ä¸­ï¼Œ{head_entity}å’Œ{tail_entity}è¡¨ç¤ºç›¸åŒçš„æ¦‚å¿µ"
        ],
        'anto': [
            f"{head_entity}å’Œ{tail_entity}æ˜¯ç›¸åçš„æ“ä½œ",
            f"{head_entity}ä¸{tail_entity}å…·æœ‰ç›¸åçš„ä½œç”¨",
            f"{head_entity}å’Œ{tail_entity}æ˜¯å¯¹ç«‹çš„æ¦‚å¿µ",
            f"åœ¨ç®—æ³•ä¸­ï¼Œ{head_entity}å’Œ{tail_entity}æ‰§è¡Œç›¸åçš„åŠŸèƒ½"
        ],
        'belg': [
            f"{head_entity}åŒ…å«{tail_entity}",
            f"{tail_entity}æ˜¯{head_entity}çš„ç»„æˆéƒ¨åˆ†",
            f"{head_entity}ç”±{tail_entity}ç­‰éƒ¨åˆ†æ„æˆ",
            f"åœ¨{head_entity}ä¸­åŒ…å«äº†{tail_entity}"
        ],
        'b-belg': [
            f"{tail_entity}åŒ…å«{head_entity}",
            f"{head_entity}æ˜¯{tail_entity}çš„ç»„æˆéƒ¨åˆ†",
            f"{tail_entity}ç”±{head_entity}ç­‰éƒ¨åˆ†æ„æˆ",
            f"åœ¨{tail_entity}ä¸­åŒ…å«äº†{head_entity}"
        ],
        'rely': [
            f"{head_entity}ä¾èµ–äº{tail_entity}",
            f"{head_entity}éœ€è¦ä½¿ç”¨{tail_entity}",
            f"{head_entity}çš„å®ç°åŸºäº{tail_entity}",
            f"å®ç°{head_entity}æ—¶éœ€è¦ä¾èµ–{tail_entity}"
        ],
        'b-rely': [
            f"{tail_entity}ä¾èµ–äº{head_entity}",
            f"{tail_entity}éœ€è¦ä½¿ç”¨{head_entity}",
            f"{tail_entity}çš„å®ç°åŸºäº{head_entity}",
            f"å®ç°{tail_entity}æ—¶éœ€è¦ä¾èµ–{head_entity}"
        ],
        'attr': [
            f"{head_entity}å…·æœ‰{tail_entity}å±æ€§",
            f"{head_entity}çš„ç‰¹å¾æ˜¯{tail_entity}",
            f"{head_entity}è¡¨ç°å‡º{tail_entity}çš„ç‰¹æ€§",
            f"{tail_entity}æ˜¯{head_entity}çš„é‡è¦å±æ€§"
        ],
        'b-attr': [
            f"{tail_entity}å…·æœ‰{head_entity}å±æ€§",
            f"{tail_entity}çš„ç‰¹å¾æ˜¯{head_entity}",
            f"{tail_entity}è¡¨ç°å‡º{head_entity}çš„ç‰¹æ€§",
            f"{head_entity}æ˜¯{tail_entity}çš„é‡è¦å±æ€§"
        ],
        'none': [
            f"{head_entity}å’Œ{tail_entity}æ²¡æœ‰ç›´æ¥å…³ç³»",
            f"{head_entity}ä¸{tail_entity}ç›¸äº’ç‹¬ç«‹",
            f"åœ¨æ•°æ®ç»“æ„ä¸­ï¼Œ{head_entity}å’Œ{tail_entity}æ˜¯ä¸åŒçš„æ¦‚å¿µ",
            f"{head_entity}å’Œ{tail_entity}åˆ†åˆ«ç”¨äºä¸åŒçš„åœºæ™¯"
        ]
    }
    
    if relation in templates:
        return random.choice(templates[relation])
    else:
        return f"{head_entity}å’Œ{tail_entity}å­˜åœ¨{relation}å…³ç³»"

def main():
    """ä¸»å‡½æ•° - é›†æˆæ–‡æœ¬æ•°æ®ç”Ÿæˆå’Œæ ‡æ³¨æ•°æ®ç”Ÿæˆ"""
    
    print("=== æ•°æ®ç»“æ„çŸ¥è¯†å›¾è°±æ„å»ºç³»ç»Ÿ ===")
    print(f"ç›®æ ‡ç”Ÿæˆæ•°é‡: {Config.NUM_RECORDS}")
    print(f"æ¯ç§å…³ç³»æœ€å°‘: {Config.MIN_PROMPTS_PER_RELATION}")
    
    # ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆå…³ç³»æç¤ºè¯ï¼ˆç”¨äºLLMæ–‡æœ¬ç”Ÿæˆï¼‰
    print("\nç¬¬ä¸€æ­¥ï¼šç”Ÿæˆå…³ç³»æç¤ºè¯...")
    prompt_tuples = generate_relation_prompts(Config.NUM_RECORDS)
    
    # ç¬¬äºŒæ­¥ï¼šè°ƒç”¨LLMç”Ÿæˆæ–‡æœ¬æ•°æ®ï¼ˆå¦‚æœéœ€è¦ï¼‰
    print("\nç¬¬äºŒæ­¥ï¼šå‡†å¤‡LLMæ–‡æœ¬ç”Ÿæˆ...")
    llm_prompts = []
    for prompt_tuple in prompt_tuples:
        if len(prompt_tuple) >= 4:
            prompt, relation_type, entity1, entity2 = prompt_tuple
            llm_prompts.append(prompt)
    
    # ä¿å­˜LLMæç¤ºè¯åˆ°æ–‡ä»¶
    with open(Config.PROMPTS_FILE, 'w', encoding='utf-8') as f:
        for prompt in llm_prompts:
            f.write(prompt + '\n')
    
    print(f"LLMæç¤ºè¯å·²ä¿å­˜åˆ°: {Config.PROMPTS_FILE}")
    print(f"å¯ä½¿ç”¨è¿™äº›æç¤ºè¯è°ƒç”¨LLMç”Ÿæˆè‡ªç„¶è¯­è¨€æ–‡æœ¬")
    
    # ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆå…³ç³»æŠ½å–æ ‡æ³¨æ–‡ä»¶
    print("\nç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆå…³ç³»æŠ½å–æ ‡æ³¨æ–‡ä»¶...")
    
    # è½¬æ¢ä¸ºæ ‡æ³¨æ•°æ®æ ¼å¼
    annotation_data = []
    for prompt_tuple in prompt_tuples:
        if len(prompt_tuple) >= 4:
            _, relation_type, entity1, entity2 = prompt_tuple
            # ç”Ÿæˆå¥å­
            sentence = generate_sentence_for_relation(relation_type, entity1, entity2)
            annotation_data.append({
                'sentence': sentence,
                'relation': relation_type,
                'head': entity1,
                'tail': entity2
            })
    
    # ç”Ÿæˆæ ‡æ³¨æ–‡ä»¶
    generate_annotation_files_from_data(annotation_data, Config.ANNOTATION_OUTPUT_DIR)
    
    # ç¬¬å››æ­¥ï¼šç»Ÿè®¡å’ŒéªŒè¯
    print("\nç¬¬å››æ­¥ï¼šæ•°æ®ç»Ÿè®¡å’ŒéªŒè¯...")
    
    # ç»Ÿè®¡å…³ç³»åˆ†å¸ƒ
    relation_stats = Counter()
    for data in annotation_data:
        relation_stats[data['relation']] += 1
    
    print(f"\n=== ç”Ÿæˆå®Œæˆ ===")
    print(f"LLMæç¤ºè¯æ•°é‡: {len(llm_prompts)}")
    print(f"æ ‡æ³¨æ•°æ®æ•°é‡: {len(annotation_data)}")
    print(f"LLMæç¤ºè¯æ–‡ä»¶: {Config.PROMPTS_FILE}")
    print(f"æ ‡æ³¨æ–‡ä»¶ç›®å½•: {Config.ANNOTATION_OUTPUT_DIR}")
    
    print("\nå…³ç³»åˆ†å¸ƒ:")
    for relation, count in sorted(relation_stats.items()):
        print(f"  {relation}: {count}")
    
    # éªŒè¯å®ä½“ä¸€è‡´æ€§
    all_entities = get_all_entities()
    print(f"\nå®ä½“ç»Ÿè®¡:")
    print(f"  æ€»å®ä½“æ•°: {len(all_entities)}")
    
    # æŒ‰ç±»åˆ«ç»Ÿè®¡
    category_stats = Counter()
    for category, entities in KNOWLEDGE_GRAPH_BASE.items():
        category_stats[category] = len(entities)
    
    print("  ç±»åˆ«åˆ†å¸ƒ:")
    for category, count in sorted(category_stats.items()):
        print(f"    {category}: {count}")
    
    print(f"\n=== ä½¿ç”¨è¯´æ˜ ===")
    print(f"1. LLMæ–‡æœ¬ç”Ÿæˆ: ä½¿ç”¨ {Config.PROMPTS_FILE} ä¸­çš„æç¤ºè¯è°ƒç”¨LLMç”Ÿæˆè‡ªç„¶è¯­è¨€æ–‡æœ¬")
    print(f"2. å…³ç³»æŠ½å–è®­ç»ƒ: ä½¿ç”¨ {Config.ANNOTATION_OUTPUT_DIR} ä¸­çš„æ ‡æ³¨æ–‡ä»¶è®­ç»ƒå…³ç³»æŠ½å–æ¨¡å‹")
    print(f"3. ä¸¤ä¸ªä»»åŠ¡å¯ä»¥ç‹¬ç«‹è¿›è¡Œï¼Œä¹Ÿå¯ä»¥ç»“åˆä½¿ç”¨")

def generate_annotation_files_from_data(annotation_data: List[Dict], output_dir: str) -> None:
    """
    ä»æ ‡æ³¨æ•°æ®ç”ŸæˆDeepKEå…³ç³»æŠ½å–æ ‡æ³¨æ–‡ä»¶
    
    Args:
        annotation_data: æ ‡æ³¨æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«sentence, relation, head, tail
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. ç”Ÿæˆå…³ç³»æ˜ å°„æ–‡ä»¶
    relation_mapping = {}
    relation_index = 0
    
    # ä¸ºæ¯ç§å…³ç³»åˆ†é…ç´¢å¼•ï¼Œä½†è·³è¿‡noneå…³ç³»
    for relation in RELATION_TYPES.keys():
        if relation != "none":
            relation_mapping[relation] = relation_index
            relation_index += 1
    
    # æ·»åŠ noneå…³ç³»ä½œä¸ºæœ€åä¸€ä¸ª
    relation_mapping["none"] = relation_index
    
    # å†™å…¥å…³ç³»æ˜ å°„æ–‡ä»¶
    relation_file = os.path.join(output_dir, "relation.csv")
    with open(relation_file, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['head_type', 'tail_type', 'relation', 'index'])
        
        for relation, index in relation_mapping.items():
            if relation == "none":
                writer.writerow(['CON', 'CON', relation, index])
            else:
                # æ ¹æ®å…³ç³»ç±»å‹ç¡®å®šå¤´å°¾å®ä½“ç±»å‹
                writer.writerow(['CON', 'CON', relation, index])
                writer.writerow(['ARI', 'ARI', relation, index])
                writer.writerow(['CON', 'ARI', relation, index])
                writer.writerow(['ARI', 'CON', relation, index])
    
    # é‡æ–°ç”Ÿæˆå…³ç³»æ˜ å°„æ–‡ä»¶ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    with open(relation_file, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['head_type', 'tail_type', 'relation', 'index'])
        
        index = 0
        for relation in RELATION_TYPES.keys():
            writer.writerow(['CON', 'ARI', relation, index])
            index += 1
        writer.writerow(['CON', 'CON', 'none', index])
    
    # 2. å‡†å¤‡è®­ç»ƒæ•°æ®
    training_data = []
    for data in annotation_data:
        sentence = data['sentence']
        relation = data['relation']
        head = data['head']
        tail = data['tail']
        
        # è®¡ç®—å®ä½“åœ¨å¥å­ä¸­çš„ä½ç½®
        head_offset = sentence.find(head)
        tail_offset = sentence.find(tail)
        
        # å¦‚æœæ‰¾ä¸åˆ°å®ä½“ä½ç½®ï¼Œè·³è¿‡è¿™æ¡æ•°æ®
        if head_offset == -1 or tail_offset == -1:
            continue
            
        training_data.append({
            'sentence': sentence,
            'relation': relation,
            'head': head,
            'tail': tail,
            'head_offset': head_offset,
            'tail_offset': tail_offset
        })
    
    # 3. éšæœºæ‰“ä¹±æ•°æ®
    random.shuffle(training_data)
    
    # 4. æŒ‰8:1:1åˆ†å‰²æ•°æ®
    total_size = len(training_data)
    train_size = int(total_size * 0.8)
    valid_size = int(total_size * 0.1)
    
    train_data = training_data[:train_size]
    valid_data = training_data[train_size:train_size + valid_size]
    test_data = training_data[train_size + valid_size:]
    
    # 5. å†™å…¥è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ–‡ä»¶
    datasets = [
        (train_data, "train.csv"),
        (valid_data, "valid.csv"),
        (test_data, "test.csv")
    ]
    
    for dataset, filename in datasets:
        filepath = os.path.join(output_dir, filename)
        with open(filepath, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['sentence', 'relation', 'head', 'tail', 'head_offset', 'tail_offset'])
            
            for item in dataset:
                writer.writerow([
                    item['sentence'],
                    item['relation'],
                    item['head'],
                    item['tail'],
                    item['head_offset'],
                    item['tail_offset']
                ])
    
    print(f"æ ‡æ³¨æ–‡ä»¶ç”Ÿæˆå®Œæˆ:")
    print(f"  å…³ç³»æ˜ å°„: {relation_file}")
    print(f"  è®­ç»ƒæ•°æ®: {len(train_data)} æ¡")
    print(f"  éªŒè¯æ•°æ®: {len(valid_data)} æ¡") 
    print(f"  æµ‹è¯•æ•°æ®: {len(test_data)} æ¡")

if __name__ == "__main__":
    main()