#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®ä½“åç§»é‡å’Œç±»å‹æå–å™¨
ä»knowledge_graph_sentences_new.jsonæ–‡ä»¶ä¸­æå–å®ä½“çš„åç§»é‡å’Œç±»å‹ä¿¡æ¯
"""

import json
import os
import csv
from typing import List, Dict, Tuple, Optional
from tqdm import tqdm


class EntityOffsetExtractor:
    """å®ä½“åç§»é‡æå–å™¨ç±»"""
    
    def __init__(self, json_file_path: str, vocab_dict_path: str = "/root/KG_inde/vocab_dict.csv"):
        """
        åˆå§‹åŒ–æå–å™¨
        
        Args:
            json_file_path: JSONæ–‡ä»¶è·¯å¾„
            vocab_dict_path: è¯å…¸æ–‡ä»¶è·¯å¾„
        """
        self.json_file_path = json_file_path
        self.vocab_dict_path = vocab_dict_path
        self.data = None
        self.entity_type_dict = {}
        self.load_vocab_dict()
        
    def load_vocab_dict(self) -> bool:
        """
        åŠ è½½è¯å…¸æ–‡ä»¶ï¼Œå»ºç«‹å®ä½“åˆ°ç±»å‹çš„æ˜ å°„
        
        Returns:
            bool: åŠ è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            with open(self.vocab_dict_path, 'r', encoding='utf-8') as f:
                csv_reader = csv.reader(f)
                for row in csv_reader:
                    if len(row) >= 2:
                        entity = row[0].strip()
                        entity_type = row[1].strip()
                        self.entity_type_dict[entity] = entity_type
            print(f"æˆåŠŸåŠ è½½è¯å…¸ï¼Œå…±æœ‰ {len(self.entity_type_dict)} ä¸ªå®ä½“ç±»å‹æ˜ å°„")
            return True
        except Exception as e:
            print(f"åŠ è½½è¯å…¸å¤±è´¥: {e}")
            return False
        
    def load_data(self) -> bool:
        """
        åŠ è½½JSONæ•°æ®
        
        Returns:
            bool: åŠ è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            with open(self.json_file_path, 'r', encoding='utf-8') as f:
                self.data = json.load(f)
            print(f"æˆåŠŸåŠ è½½æ•°æ®ï¼Œå…±æœ‰ {len(self.data['sentences'])} æ¡å¥å­")
            return True
        except Exception as e:
            print(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return False
    
    def find_entity_offset(self, sentence: str, entity: str) -> List[int]:
        """
        æŸ¥æ‰¾å®ä½“åœ¨å¥å­ä¸­çš„æ‰€æœ‰åç§»ä½ç½®
        
        Args:
            sentence: å¥å­æ–‡æœ¬
            entity: å®ä½“æ–‡æœ¬
            
        Returns:
            List[int]: æ‰€æœ‰åŒ¹é…ä½ç½®çš„èµ·å§‹åç§»é‡åˆ—è¡¨
        """
        if not entity or not sentence:
            return []
        
        offsets = []
        start = 0
        while True:
            pos = sentence.find(entity, start)
            if pos == -1:
                break
            offsets.append(pos)
            start = pos + 1
        
        return offsets
    
    def get_entity_type(self, entity: str) -> str:
        """
        æ ¹æ®è¯å…¸è·å–å®ä½“ç±»å‹
        
        Args:
            entity: å®ä½“æ–‡æœ¬
            
        Returns:
            str: å®ä½“ç±»å‹ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›"ENTITY"
        """
        return self.entity_type_dict.get(entity, "ENTITY")
    
    def extract_entity_info(self, sentence_item: Dict) -> Optional[Dict]:
        """
        ä»å•ä¸ªå¥å­é¡¹ä¸­æå–å®ä½“ä¿¡æ¯
        
        Args:
            sentence_item: åŒ…å«å¥å­å’Œå®ä½“ä¿¡æ¯çš„å­—å…¸
            
        Returns:
            Dict: æå–çš„å®ä½“ä¿¡æ¯ï¼ŒåŒ…å«åç§»é‡å’Œç±»å‹
        """
        sentence = sentence_item.get('sentence', '').strip()
        entity1 = sentence_item.get('entity1', '').strip()
        entity2 = sentence_item.get('entity2', '').strip()
        relation = sentence_item.get('relation', '').strip()
        
        if not sentence or not entity1 or not entity2:
            return None
        
        # æŸ¥æ‰¾å®ä½“åç§»é‡
        entity1_offsets = self.find_entity_offset(sentence, entity1)
        entity2_offsets = self.find_entity_offset(sentence, entity2)
        
        if not entity1_offsets or not entity2_offsets:
            return None
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªåŒ¹é…çš„åç§»é‡
        entity1_offset = entity1_offsets[0]
        entity2_offset = entity2_offsets[0]
        
        # ç¡®ä¿å¤´å®ä½“åœ¨å°¾å®ä½“ä¹‹å‰
        if entity1_offset <= entity2_offset:
            head_entity = entity1
            tail_entity = entity2
            head_offset = entity1_offset
            tail_offset = entity2_offset
        else:
            head_entity = entity2
            tail_entity = entity1
            head_offset = entity2_offset
            tail_offset = entity1_offset
        
        return {
            "sentence": sentence,
            "head": head_entity,
            "tail": tail_entity,
            "head_offset": head_offset,
            "tail_offset": tail_offset,
            "head_type": self.get_entity_type(head_entity),
            "tail_type": self.get_entity_type(tail_entity),
            "relation": relation,
            "head_end_offset": head_offset + len(head_entity),
            "tail_end_offset": tail_offset + len(tail_entity)
        }
    
    def extract_all_entities(self) -> List[Dict]:
        """
        æå–æ‰€æœ‰å®ä½“çš„åç§»é‡å’Œç±»å‹ä¿¡æ¯
        
        Returns:
            List[Dict]: åŒ…å«æ‰€æœ‰å®ä½“ä¿¡æ¯çš„åˆ—è¡¨
        """
        if not self.data:
            print("æ•°æ®æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_data()")
            return []
        
        results = []
        failed_count = 0
        
        print("å¼€å§‹æå–å®ä½“åç§»é‡å’Œç±»å‹ä¿¡æ¯...")
        for item in tqdm(self.data['sentences'], desc="å¤„ç†è¿›åº¦"):
            entity_info = self.extract_entity_info(item)
            if entity_info:
                results.append(entity_info)
            else:
                failed_count += 1
        
        print(f"æå–å®Œæˆï¼æˆåŠŸ: {len(results)}, å¤±è´¥: {failed_count}")
        return results
    
    def save_results(self, results: List[Dict], output_path: str) -> bool:
        """
        ä¿å­˜æå–ç»“æœåˆ°æ–‡ä»¶
        
        Args:
            results: æå–çš„ç»“æœåˆ—è¡¨
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            print(f"ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
            print(f"å…±ä¿å­˜ {len(results)} æ¡è®°å½•")
            return True
        except Exception as e:
            print(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
            return False
    
    def generate_statistics(self, results: List[Dict]) -> Dict:
        """
        ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        
        Args:
            results: æå–çš„ç»“æœåˆ—è¡¨
            
        Returns:
            Dict: ç»Ÿè®¡ä¿¡æ¯
        """
        if not results:
            return {}
        
        relations = [item['relation'] for item in results]
        unique_relations = set(relations)
        
        head_entities = [item['head'] for item in results]
        tail_entities = [item['tail'] for item in results]
        all_entities = set(head_entities + tail_entities)
        
        stats = {
            "total_records": len(results),
            "unique_relations": len(unique_relations),
            "unique_entities": len(all_entities),
            "relation_distribution": {rel: relations.count(rel) for rel in unique_relations},
            "average_sentence_length": sum(len(item['sentence']) for item in results) / len(results)
        }
        
        return stats


def main():
    """ä¸»å‡½æ•°"""
    # æ–‡ä»¶è·¯å¾„é…ç½®
    json_file_path = "/root/KG_inde/DeepKE/example/ner/standard/w2ner/data/knowledge_graph_sentences_new.json"
    output_dir = "/root/KG_inde/output_predict"
    output_file = os.path.join(output_dir, "entity_offsets.json")
    stats_file = os.path.join(output_dir, "extraction_statistics.json")
    
    # åˆ›å»ºæå–å™¨å®ä¾‹
    extractor = EntityOffsetExtractor(json_file_path)
    
    # åŠ è½½æ•°æ®
    if not extractor.load_data():
        return
    
    # æå–å®ä½“ä¿¡æ¯
    results = extractor.extract_all_entities()
    
    if not results:
        print("æ²¡æœ‰æå–åˆ°ä»»ä½•å®ä½“ä¿¡æ¯")
        return
    
    # ä¿å­˜ç»“æœ
    if extractor.save_results(results, output_file):
        print("âœ… å®ä½“åç§»é‡æå–å®Œæˆ")
    
    # ç”Ÿæˆå¹¶ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats = extractor.generate_statistics(results)
    if stats:
        try:
            os.makedirs(output_dir, exist_ok=True)
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_file}")
            
            # æ‰“å°å…³é”®ç»Ÿè®¡ä¿¡æ¯
            print("\n=== æå–ç»Ÿè®¡ ===")
            print(f"æ€»è®°å½•æ•°: {stats['total_records']}")
            print(f"å”¯ä¸€å…³ç³»æ•°: {stats['unique_relations']}")
            print(f"å”¯ä¸€å®ä½“æ•°: {stats['unique_entities']}")
            print(f"å¹³å‡å¥å­é•¿åº¦: {stats['average_sentence_length']:.1f} å­—ç¬¦")
            
        except Exception as e:
            print(f"ä¿å­˜ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")


if __name__ == "__main__":
    main()