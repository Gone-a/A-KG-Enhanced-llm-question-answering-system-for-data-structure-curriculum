#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çŸ¥è¯†å›¾è°±é«˜çº§è´¨é‡ä¼˜åŒ–å™¨
ä¿®æ­£è¯­ä¹‰ä¸ä¸€è‡´çš„å…³ç³»ï¼Œä¸ºå…³ç³»æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
"""

import json
import os
import re
from neo4j import GraphDatabase
from collections import defaultdict
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AdvancedKnowledgeGraphOptimizer:
    def __init__(self, uri="bolt://localhost:7687", user="neo4j", password="123456"):
        # ä»ç¯å¢ƒå˜é‡è·å–å¯†ç 
        password = os.getenv("NEO4J_KEY", password)
        self.driver = GraphDatabase.driver(uri, auth=(user, password))
        self.data_dir = "/root/KG_inde/neo4j/data"
        
        # åŠ è½½è´¨é‡åˆ†ææŠ¥å‘Š
        self.load_quality_report()
        
        # å®šä¹‰è¯­ä¹‰ä¿®æ­£è§„åˆ™
        self.semantic_correction_rules = {
            # åˆ é™¤è¯­ä¹‰ä¸åˆç†çš„å…³ç³»
            "delete_rules": [
                # ApplicationScenario -> DataStructure (åº”ç”¨åœºæ™¯ä¸åº”è¯¥æŒ‡å‘æ•°æ®ç»“æ„)
                ("ApplicationScenario", "appliesTo", "DataStructure"),
                # Operation -> Algorithm (æ“ä½œä¸åº”è¯¥åº”ç”¨åˆ°ç®—æ³•)
                ("Operation", "appliesTo", "Algorithm"),
                # Complexity -> DataStructure (å¤æ‚åº¦ä¸åº”è¯¥åº”ç”¨åˆ°æ•°æ®ç»“æ„)
                ("Complexity", "appliesTo", "DataStructure"),
                # å…¶ä»–ä¸åˆç†çš„ç»„åˆ
                ("ApplicationScenario", "uses", "Algorithm"),
                ("Complexity", "uses", "DataStructure")
            ],
            # ä¿®æ­£å…³ç³»ç±»å‹
            "correction_rules": [
                # DataStructure -> ApplicationScenario åº”è¯¥æ˜¯ appliesTo
                ("DataStructure", "uses", "ApplicationScenario", "appliesTo"),
                # Algorithm -> ApplicationScenario åº”è¯¥æ˜¯ appliesTo
                ("Algorithm", "uses", "ApplicationScenario", "appliesTo"),
                # DataStructure -> Operation åº”è¯¥æ˜¯ appliesTo
                ("DataStructure", "uses", "Operation", "appliesTo")
            ]
        }
        
        # ä¸Šä¸‹æ–‡ç”Ÿæˆæ¨¡æ¿
        self.context_templates = {
            "appliesTo": {
                ("DataStructure", "ApplicationScenario"): "{source}æ•°æ®ç»“æ„é€‚ç”¨äº{target}åœºæ™¯",
                ("Algorithm", "ApplicationScenario"): "{source}ç®—æ³•é€‚ç”¨äº{target}åœºæ™¯",
                ("DataStructure", "Operation"): "{source}æ•°æ®ç»“æ„æ”¯æŒ{target}æ“ä½œ",
                ("Algorithm", "Operation"): "{source}ç®—æ³•æ”¯æŒ{target}æ“ä½œ"
            },
            "uses": {
                ("Algorithm", "DataStructure"): "{source}ç®—æ³•ä½¿ç”¨{target}æ•°æ®ç»“æ„",
                ("Operation", "DataStructure"): "{target}æ•°æ®ç»“æ„ç”¨äº{source}æ“ä½œ",
                ("DataStructure", "DataStructure"): "{source}åŸºäº{target}å®ç°"
            },
            "variantOf": {
                ("DataStructure", "DataStructure"): "{source}æ˜¯{target}çš„å˜ä½“",
                ("Algorithm", "Algorithm"): "{source}æ˜¯{target}çš„å˜ä½“"
            },
            "hasComplexity": {
                ("Algorithm", "Complexity"): "{source}ç®—æ³•çš„æ—¶é—´å¤æ‚åº¦ä¸º{target}",
                ("Operation", "Complexity"): "{source}æ“ä½œçš„æ—¶é—´å¤æ‚åº¦ä¸º{target}"
            }
        }
    
    def close(self):
        self.driver.close()
    
    def load_quality_report(self):
        """åŠ è½½è´¨é‡åˆ†ææŠ¥å‘Š"""
        try:
            report_file = os.path.join(self.data_dir, "quality_analysis_report.json")
            with open(report_file, 'r', encoding='utf-8') as f:
                self.quality_report = json.load(f)
            logger.info("è´¨é‡åˆ†ææŠ¥å‘ŠåŠ è½½å®Œæˆ")
        except Exception as e:
            logger.error(f"åŠ è½½è´¨é‡åˆ†ææŠ¥å‘Šå¤±è´¥: {e}")
            self.quality_report = {}
    
    def get_node_type(self, node_name):
        """è·å–èŠ‚ç‚¹ç±»å‹"""
        with self.driver.session() as session:
            result = session.run("""
                MATCH (n {name: $name})
                RETURN labels(n) as labels
            """, name=node_name)
            
            record = result.single()
            if record and record["labels"]:
                return record["labels"][0]
            return "Unknown"
    
    def fix_semantic_inconsistencies(self):
        """ä¿®æ­£è¯­ä¹‰ä¸ä¸€è‡´çš„å…³ç³»"""
        logger.info("å¼€å§‹ä¿®æ­£è¯­ä¹‰ä¸ä¸€è‡´çš„å…³ç³»...")
        
        deleted_count = 0
        corrected_count = 0
        
        with self.driver.session() as session:
            # è·å–æ‰€æœ‰å…³ç³»åŠå…¶èŠ‚ç‚¹ç±»å‹
            result = session.run("""
                MATCH (a)-[r]->(b)
                RETURN a.name as source, b.name as target, 
                       type(r) as relation, labels(a) as source_labels, 
                       labels(b) as target_labels, id(r) as rel_id
            """)
            
            relationships = result.data()
            
            for rel in relationships:
                source_type = rel["source_labels"][0] if rel["source_labels"] else "Unknown"
                target_type = rel["target_labels"][0] if rel["target_labels"] else "Unknown"
                relation_type = rel["relation"]
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ é™¤
                delete_pattern = (source_type, relation_type, target_type)
                if delete_pattern in self.semantic_correction_rules["delete_rules"]:
                    session.run("""
                        MATCH ()-[r]->()
                        WHERE id(r) = $rel_id
                        DELETE r
                    """, rel_id=rel["rel_id"])
                    deleted_count += 1
                    logger.debug(f"åˆ é™¤ä¸åˆç†å…³ç³»: {rel['source']} --[{relation_type}]--> {rel['target']}")
                    continue
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿®æ­£å…³ç³»ç±»å‹
                for rule in self.semantic_correction_rules["correction_rules"]:
                    if (source_type, relation_type, target_type) == rule[:3]:
                        new_relation = rule[3]
                        session.run("""
                            MATCH (a)-[r]->(b)
                            WHERE id(r) = $rel_id
                            CREATE (a)-[new_r:""" + new_relation + """]->(b)
                            SET new_r = properties(r)
                            DELETE r
                        """, rel_id=rel["rel_id"])
                        corrected_count += 1
                        logger.debug(f"ä¿®æ­£å…³ç³»ç±»å‹: {rel['source']} --[{relation_type} -> {new_relation}]--> {rel['target']}")
                        break
        
        logger.info(f"è¯­ä¹‰ä¿®æ­£å®Œæˆ: åˆ é™¤ {deleted_count} ä¸ªä¸åˆç†å…³ç³», ä¿®æ­£ {corrected_count} ä¸ªå…³ç³»ç±»å‹")
        return deleted_count, corrected_count
    
    def add_missing_context(self):
        """ä¸ºç¼ºå°‘ä¸Šä¸‹æ–‡çš„å…³ç³»æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        logger.info("å¼€å§‹ä¸ºå…³ç³»æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯...")
        
        added_count = 0
        
        with self.driver.session() as session:
            # è·å–ç¼ºå°‘source_sentenceçš„å…³ç³»
            result = session.run("""
                MATCH (a)-[r]->(b)
                WHERE r.source_sentence IS NULL
                RETURN a.name as source, b.name as target, 
                       type(r) as relation, labels(a) as source_labels, 
                       labels(b) as target_labels, id(r) as rel_id
            """)
            
            relationships = result.data()
            
            for rel in relationships:
                source_type = rel["source_labels"][0] if rel["source_labels"] else "Unknown"
                target_type = rel["target_labels"][0] if rel["target_labels"] else "Unknown"
                relation_type = rel["relation"]
                
                # ç”Ÿæˆä¸Šä¸‹æ–‡å¥å­
                context = self.generate_context(
                    rel["source"], rel["target"], 
                    source_type, target_type, relation_type
                )
                
                if context:
                    session.run("""
                        MATCH ()-[r]->()
                        WHERE id(r) = $rel_id
                        SET r.source_sentence = $context
                    """, rel_id=rel["rel_id"], context=context)
                    added_count += 1
                    logger.debug(f"æ·»åŠ ä¸Šä¸‹æ–‡: {rel['source']} --[{relation_type}]--> {rel['target']}: {context}")
        
        logger.info(f"ä¸Šä¸‹æ–‡æ·»åŠ å®Œæˆ: ä¸º {added_count} ä¸ªå…³ç³»æ·»åŠ äº†ä¸Šä¸‹æ–‡ä¿¡æ¯")
        return added_count
    
    def generate_context(self, source, target, source_type, target_type, relation_type):
        """ç”Ÿæˆå…³ç³»çš„ä¸Šä¸‹æ–‡å¥å­"""
        # æŸ¥æ‰¾åŒ¹é…çš„æ¨¡æ¿
        if relation_type in self.context_templates:
            type_templates = self.context_templates[relation_type]
            type_key = (source_type, target_type)
            
            if type_key in type_templates:
                template = type_templates[type_key]
                return template.format(source=source, target=target)
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…çš„æ¨¡æ¿ï¼Œç”Ÿæˆé€šç”¨ä¸Šä¸‹æ–‡
        generic_templates = {
            "appliesTo": f"{source}é€‚ç”¨äº{target}",
            "uses": f"{source}ä½¿ç”¨{target}",
            "variantOf": f"{source}æ˜¯{target}çš„å˜ä½“",
            "hasComplexity": f"{source}çš„å¤æ‚åº¦ä¸º{target}"
        }
        
        return generic_templates.get(relation_type, f"{source}ä¸{target}å­˜åœ¨{relation_type}å…³ç³»")
    
    def optimize_hub_nodes(self):
        """ä¼˜åŒ–hubèŠ‚ç‚¹ï¼ˆé«˜è¿æ¥åº¦èŠ‚ç‚¹ï¼‰"""
        logger.info("å¼€å§‹ä¼˜åŒ–hubèŠ‚ç‚¹...")
        
        optimized_count = 0
        
        with self.driver.session() as session:
            # æ‰¾å‡ºè¿æ¥åº¦è¿‡é«˜çš„èŠ‚ç‚¹
            result = session.run("""
                MATCH (n)
                WITH n, COUNT {(n)--()} as degree
                WHERE degree > 15
                RETURN n.name as name, degree
                ORDER BY degree DESC
            """)
            
            hub_nodes = result.data()
            
            for hub in hub_nodes:
                node_name = hub["name"]
                degree = hub["degree"]
                
                # åˆ†æhubèŠ‚ç‚¹çš„å…³ç³»ç±»å‹åˆ†å¸ƒ
                rel_analysis = session.run("""
                    MATCH (n {name: $name})-[r]-()
                    RETURN type(r) as relation_type, count(*) as count
                    ORDER BY count DESC
                """, name=node_name).data()
                
                # å¦‚æœæŸä¸ªå…³ç³»ç±»å‹å æ¯”è¿‡é«˜ï¼Œå¯èƒ½éœ€è¦ä¼˜åŒ–
                total_rels = sum(item["count"] for item in rel_analysis)
                for rel_type_info in rel_analysis:
                    rel_type = rel_type_info["relation_type"]
                    count = rel_type_info["count"]
                    ratio = count / total_rels
                    
                    # å¦‚æœæŸç§å…³ç³»ç±»å‹å æ¯”è¶…è¿‡70%ï¼Œè€ƒè™‘æ˜¯å¦åˆç†
                    if ratio > 0.7 and count > 10:
                        logger.info(f"HubèŠ‚ç‚¹ {node_name} çš„ {rel_type} å…³ç³»å æ¯”è¿‡é«˜: {ratio:.2%} ({count}/{total_rels})")
                        # è¿™é‡Œå¯ä»¥æ·»åŠ å…·ä½“çš„ä¼˜åŒ–é€»è¾‘
                        optimized_count += 1
        
        logger.info(f"HubèŠ‚ç‚¹ä¼˜åŒ–å®Œæˆ: åˆ†æäº† {len(hub_nodes)} ä¸ªhubèŠ‚ç‚¹")
        return optimized_count
    
    def validate_optimizations(self):
        """éªŒè¯ä¼˜åŒ–ç»“æœ"""
        logger.info("å¼€å§‹éªŒè¯ä¼˜åŒ–ç»“æœ...")
        
        with self.driver.session() as session:
            # ç»Ÿè®¡ä¼˜åŒ–åçš„æ•°æ®
            node_count = session.run("MATCH (n) RETURN count(n) as count").single()["count"]
            rel_count = session.run("MATCH ()-[r]->() RETURN count(r) as count").single()["count"]
            
            # ç»Ÿè®¡æœ‰ä¸Šä¸‹æ–‡çš„å…³ç³»æ•°é‡
            context_count = session.run("""
                MATCH ()-[r]->()
                WHERE r.source_sentence IS NOT NULL
                RETURN count(r) as count
            """).single()["count"]
            
            # ç»Ÿè®¡å…³ç³»ç±»å‹åˆ†å¸ƒ
            rel_types = session.run("""
                MATCH ()-[r]->()
                RETURN type(r) as type, count(*) as count
                ORDER BY count DESC
            """).data()
            
            validation_result = {
                "total_nodes": node_count,
                "total_relationships": rel_count,
                "relationships_with_context": context_count,
                "context_coverage": context_count / rel_count if rel_count > 0 else 0,
                "relationship_types": rel_types
            }
            
            logger.info(f"éªŒè¯ç»“æœ: {node_count} ä¸ªèŠ‚ç‚¹, {rel_count} ä¸ªå…³ç³»")
            logger.info(f"ä¸Šä¸‹æ–‡è¦†ç›–ç‡: {validation_result['context_coverage']:.2%}")
            
            return validation_result
    
    def run_optimization(self):
        """è¿è¡Œå®Œæ•´çš„ä¼˜åŒ–æµç¨‹"""
        logger.info("å¼€å§‹è¿è¡Œé«˜çº§è´¨é‡ä¼˜åŒ–...")
        
        try:
            # 1. ä¿®æ­£è¯­ä¹‰ä¸ä¸€è‡´çš„å…³ç³»
            deleted, corrected = self.fix_semantic_inconsistencies()
            
            # 2. æ·»åŠ ç¼ºå°‘çš„ä¸Šä¸‹æ–‡
            context_added = self.add_missing_context()
            
            # 3. ä¼˜åŒ–hubèŠ‚ç‚¹
            hub_optimized = self.optimize_hub_nodes()
            
            # 4. éªŒè¯ä¼˜åŒ–ç»“æœ
            validation = self.validate_optimizations()
            
            # 5. ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
            optimization_report = {
                "optimization_time": "2025-10-01T02:56:00",
                "actions_taken": {
                    "semantic_corrections": {
                        "deleted_relationships": deleted,
                        "corrected_relationships": corrected
                    },
                    "context_enhancement": {
                        "relationships_with_context_added": context_added
                    },
                    "hub_optimization": {
                        "hub_nodes_analyzed": hub_optimized
                    }
                },
                "final_statistics": validation,
                "improvements": {
                    "context_coverage_improved": f"{validation['context_coverage']:.2%}",
                    "semantic_consistency_improved": deleted + corrected > 0,
                    "total_relationships_after_optimization": validation["total_relationships"]
                }
            }
            
            # ä¿å­˜ä¼˜åŒ–æŠ¥å‘Š
            report_file = os.path.join(self.data_dir, "optimization_report.json")
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(optimization_report, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ä¼˜åŒ–æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
            
            # æ‰“å°ä¼˜åŒ–æ‘˜è¦
            self.print_optimization_summary(optimization_report)
            
            return optimization_report
            
        except Exception as e:
            logger.error(f"ä¼˜åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            raise
    
    def print_optimization_summary(self, report):
        """æ‰“å°ä¼˜åŒ–æ‘˜è¦"""
        print("\n" + "="*60)
        print("çŸ¥è¯†å›¾è°±é«˜çº§è´¨é‡ä¼˜åŒ–æŠ¥å‘Š")
        print("="*60)
        
        actions = report["actions_taken"]
        stats = report["final_statistics"]
        
        print(f"\nğŸ”§ ä¼˜åŒ–æ“ä½œ:")
        print(f"  è¯­ä¹‰ä¿®æ­£:")
        print(f"    - åˆ é™¤ä¸åˆç†å…³ç³»: {actions['semantic_corrections']['deleted_relationships']} ä¸ª")
        print(f"    - ä¿®æ­£å…³ç³»ç±»å‹: {actions['semantic_corrections']['corrected_relationships']} ä¸ª")
        print(f"  ä¸Šä¸‹æ–‡å¢å¼º:")
        print(f"    - æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯: {actions['context_enhancement']['relationships_with_context_added']} ä¸ªå…³ç³»")
        print(f"  HubèŠ‚ç‚¹ä¼˜åŒ–:")
        print(f"    - åˆ†æHubèŠ‚ç‚¹: {actions['hub_optimization']['hub_nodes_analyzed']} ä¸ª")
        
        print(f"\nğŸ“Š ä¼˜åŒ–åç»Ÿè®¡:")
        print(f"  èŠ‚ç‚¹æ€»æ•°: {stats['total_nodes']}")
        print(f"  å…³ç³»æ€»æ•°: {stats['total_relationships']}")
        print(f"  ä¸Šä¸‹æ–‡è¦†ç›–ç‡: {stats['context_coverage']:.2%}")
        
        print(f"\nâœ… æ”¹è¿›æ•ˆæœ:")
        improvements = report["improvements"]
        print(f"  ä¸Šä¸‹æ–‡è¦†ç›–ç‡æå‡è‡³: {improvements['context_coverage_improved']}")
        print(f"  è¯­ä¹‰ä¸€è‡´æ€§æ”¹è¿›: {'æ˜¯' if improvements['semantic_consistency_improved'] else 'å¦'}")
        print(f"  ä¼˜åŒ–åå…³ç³»æ€»æ•°: {improvements['total_relationships_after_optimization']}")

def main():
    optimizer = AdvancedKnowledgeGraphOptimizer()
    try:
        report = optimizer.run_optimization()
        return report
    finally:
        optimizer.close()

if __name__ == "__main__":
    main()