from py2neo import Graph, Node, Relationship
import os
import pandas as pd
import re
import json
from tqdm import tqdm
import csv

class Neo4jKnowledgeGraph:
    def __init__(self, uri="bolt://localhost:7687", user="neo4j", password="123456", confidence=0.7):
        """åˆå§‹åŒ–Neo4jå›¾æ•°æ®åº“è¿æ¥
        
        Args:
            uri (str): Neo4jæ•°æ®åº“URI
            user (str): ç”¨æˆ·å
            password (str): å¯†ç 
            confidence (float): ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œé»˜è®¤0.7
        """
        self.confidence = confidence
        # è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
        cur_dir = os.path.dirname(os.path.abspath(__file__))
        self.data_path = os.path.join(cur_dir, "data")
        
        # Neo4jè¿æ¥é…ç½®
        try:
            #ä»ç¯å¢ƒå˜é‡è·å–è¿æ¥ä¿¡æ¯
            password = os.getenv("NEO4J_KEY", password)
            self.graph = Graph(uri, auth=(user, password))
            # æµ‹è¯•è¿æ¥
            self.graph.run("RETURN 1")
            print("âœ… Neo4jå›¾æ•°æ®åº“è¿æ¥æˆåŠŸ")
        except Exception as e:
            print(f"âŒ Neo4jè¿æ¥å¤±è´¥: {e}")
            raise e
        
        # ä»CSVæ–‡ä»¶åŠ è½½å®ä½“ç±»å‹æ˜ å°„
        self.entity_type_dict = self.load_entity_types_from_csv()
        
        # ä»CSVæ–‡ä»¶åŠ è½½å…³ç³»ç±»å‹æ˜ å°„
        self.relation_dict = self.load_relations_from_csv()
        
        # å¤‡ç”¨çš„å®ä½“ç±»å‹æ˜ å°„ï¼ˆç”¨äºæœªåœ¨CSVä¸­å®šä¹‰çš„å®ä½“ï¼‰
        self.fallback_entity_type_dict = {
            "ApplicationScenario": "åº”ç”¨åœºæ™¯",
            "DataStructure": "æ•°æ®ç»“æ„", 
            "Algorithm": "ç®—æ³•",
            "Operation": "æ“ä½œ",
            "Complexity": "å¤æ‚åº¦",
            "PrincipleOrProperty": "åŸç†æˆ–å±æ€§"
        }
        
        # å¤‡ç”¨çš„å…³ç³»ç±»å‹æ˜ å°„
        self.fallback_relation_dict = {
            "hasComplexity": "å…·æœ‰å¤æ‚åº¦",
            "uses": "ä½¿ç”¨",
            "variantOf": "æ˜¯å˜ä½“",
            "appliesTo": "é€‚ç”¨äº",
            "provides": "æä¾›",
            "implementedAs": "å®ç°ä¸º",
            "usedIn": "ç”¨äº"
        }

    def load_entity_types_from_csv(self):
        """ä»vocab_dict.csvæ–‡ä»¶åŠ è½½å®ä½“ç±»å‹æ˜ å°„"""
        entity_types = {}
        vocab_dict_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'vocab_dict.csv')
        
        try:
            with open(vocab_dict_path, 'r', encoding='utf-8') as f:
                reader = csv.reader(f)
                for row in reader:
                    if len(row) >= 2:
                        entity_name = row[0].strip()
                        entity_type = row[1].strip()
                        entity_types[entity_name] = entity_type
            print(f"æˆåŠŸåŠ è½½ {len(entity_types)} ä¸ªå®ä½“ç±»å‹æ˜ å°„")
        except FileNotFoundError:
            print(f"è­¦å‘Šï¼šæœªæ‰¾åˆ°vocab_dict.csvæ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤å®ä½“ç±»å‹æ˜ å°„")
            return {}
        except Exception as e:
            print(f"åŠ è½½vocab_dict.csvæ—¶å‡ºé”™ï¼š{e}")
            return {}
            
        return entity_types

    def load_relations_from_csv(self):
        """ä»relation.csvæ–‡ä»¶åŠ è½½å…³ç³»ç±»å‹æ˜ å°„"""
        relations = {}
        relation_csv_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'relation.csv')
        
        try:
            with open(relation_csv_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    relation_name = row['relation'].strip()
                    head_type = row['head_type'].strip()
                    tail_type = row['tail_type'].strip()
                    index = int(row['index'])
                    
                    # å­˜å‚¨å…³ç³»ä¿¡æ¯ï¼ŒåŒ…æ‹¬å¤´å°¾å®ä½“ç±»å‹çº¦æŸ
                    relations[relation_name] = {
                        'name': relation_name,
                        'head_type': head_type,
                        'tail_type': tail_type,
                        'index': index
                    }
            print(f"æˆåŠŸåŠ è½½ {len(relations)} ä¸ªå…³ç³»ç±»å‹æ˜ å°„")
        except FileNotFoundError:
            print(f"è­¦å‘Šï¼šæœªæ‰¾åˆ°relation.csvæ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤å…³ç³»ç±»å‹æ˜ å°„")
            return {}
        except Exception as e:
            print(f"åŠ è½½relation.csvæ—¶å‡ºé”™ï¼š{e}")
            return {}
            
        return relations

    def clean_database(self):
        """å½»åº•æ¸…ç†æ•°æ®åº“ï¼šåˆ é™¤æ‰€æœ‰èŠ‚ç‚¹ã€å…³ç³»ã€ç´¢å¼•ã€çº¦æŸå’Œå±æ€§æ ‡ç­¾"""
        try:
            print("ğŸ§¹ å¼€å§‹æ¸…ç†æ•°æ®åº“...")
            
            # 1. è·å–å¹¶åˆ é™¤æ‰€æœ‰çº¦æŸ
            constraints_result = self.graph.run("SHOW CONSTRAINTS").data()
            for constraint in constraints_result:
                constraint_name = constraint.get('name')
                if constraint_name:
                    try:
                        self.graph.run(f"DROP CONSTRAINT {constraint_name} IF EXISTS")
                        print(f"   âœ“ åˆ é™¤çº¦æŸ: {constraint_name}")
                    except Exception as e:
                        print(f"   âš ï¸ åˆ é™¤çº¦æŸ {constraint_name} å¤±è´¥: {e}")
            
            # 2. è·å–å¹¶åˆ é™¤æ‰€æœ‰ç´¢å¼•
            indexes_result = self.graph.run("SHOW INDEXES").data()
            for index in indexes_result:
                index_name = index.get('name')
                if index_name:
                    try:
                        self.graph.run(f"DROP INDEX {index_name} IF EXISTS")
                        print(f"   âœ“ åˆ é™¤ç´¢å¼•: {index_name}")
                    except Exception as e:
                        print(f"   âš ï¸ åˆ é™¤ç´¢å¼• {index_name} å¤±è´¥: {e}")
            
            # 3. åˆ é™¤æ‰€æœ‰èŠ‚ç‚¹å’Œå…³ç³»
            self.graph.run("MATCH (n) DETACH DELETE n")
            print("   âœ“ åˆ é™¤æ‰€æœ‰èŠ‚ç‚¹å’Œå…³ç³»")
            
            # 4. å°è¯•æ¸…ç†å±æ€§æ ‡ç­¾å…ƒæ•°æ® - Neo4jç‰¹æ®Šå¤„ç†
            print("   ğŸ”„ å°è¯•æ¸…ç†å±æ€§æ ‡ç­¾å…ƒæ•°æ®...")
            
            # ç­–ç•¥1: å°è¯•ä½¿ç”¨APOCæ¸…ç†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            try:
                self.graph.run("CALL apoc.schema.assert({}, {})")
                print("   âœ“ ä½¿ç”¨APOCæ¸…ç†æ¨¡å¼")
            except:
                pass
            
            # ç­–ç•¥2: å°è¯•æ¸…ç†ç³»ç»Ÿç¼“å­˜
            try:
                self.graph.run("CALL db.clearQueryCaches()")
                print("   âœ“ æ¸…ç†æŸ¥è¯¢ç¼“å­˜")
            except:
                pass
            
            # ç­–ç•¥3: å¤šæ¬¡åˆ é™¤æ“ä½œ
            for i in range(5):
                self.graph.run("MATCH (n) DETACH DELETE n")
            
            # ç­–ç•¥4: å°è¯•æ‰‹åŠ¨æ¸…ç†ç»Ÿè®¡ä¿¡æ¯
            try:
                self.graph.run("CALL db.stats.clear()")
                print("   âœ“ æ¸…ç†ç»Ÿè®¡ä¿¡æ¯")
            except:
                pass
            
            # ç­–ç•¥5: é‡æ–°å»ºç«‹è¿æ¥ï¼ˆä¿®å¤å±æ€§å¼•ç”¨é—®é¢˜ï¼‰
            try:
                # ä¿å­˜è¿æ¥å‚æ•°
                uri = getattr(self, 'uri', "bolt://localhost:7687")
                user = getattr(self, 'user', "neo4j") 
                password = getattr(self, 'password', "123456")
                
                # é‡æ–°è¿æ¥
                from py2neo import Graph
                self.graph = Graph(uri, auth=(user, password))
                print("   âœ“ é‡æ–°è¿æ¥æ•°æ®åº“åˆ·æ–°å…ƒæ•°æ®")
            except Exception as e:
                print(f"   âš ï¸ é‡æ–°è¿æ¥å¤±è´¥: {e}")
            
            # 5. éªŒè¯æ¸…ç†ç»“æœ
            try:
                property_keys_result = self.graph.run("CALL db.propertyKeys()").data()
                labels_result = self.graph.run("CALL db.labels()").data()
                rel_types_result = self.graph.run("CALL db.relationshipTypes()").data()
                
                property_count = len(property_keys_result) if property_keys_result else 0
                label_count = len(labels_result) if labels_result else 0
                rel_type_count = len(rel_types_result) if rel_types_result else 0
                
                if property_count == 0 and label_count == 0 and rel_type_count == 0:
                    print("   âœ… å±æ€§æ ‡ç­¾å·²å®Œå…¨æ¸…ç†")
                else:
                    print(f"   âš ï¸ Neo4jå…ƒæ•°æ®æ®‹ç•™ï¼ˆè¿™æ˜¯æ­£å¸¸çš„ï¼‰ï¼šå±æ€§é”®{property_count}ä¸ªï¼Œæ ‡ç­¾{label_count}ä¸ªï¼Œå…³ç³»ç±»å‹{rel_type_count}ä¸ª")
                    print("   â„¹ï¸ æ³¨æ„ï¼šNeo4jä¼šä¿ç•™å±æ€§é”®å…ƒæ•°æ®ç›´åˆ°æ•°æ®åº“é‡å¯ï¼Œè¿™ä¸å½±å“æ–°æ•°æ®çš„åˆ›å»º")
                        
            except Exception as e:
                print(f"   âš ï¸ éªŒè¯æ¸…ç†ç»“æœæ—¶å‡ºé”™: {e}")
            
            print("ğŸ§¹ğŸ§¹ æ•°æ®åº“å·²å½»åº•æ¸…ç†å®Œæˆï¼ˆåŒ…æ‹¬å±æ€§æ ‡ç­¾ï¼‰")
            
        except Exception as e:
            print(f"æ¸…ç†æ•°æ®åº“æ—¶å‡ºé”™: {e}")
            # å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨ä¼ ç»Ÿæ–¹å¼æ¸…ç†
            print("âš ï¸ å°è¯•å¤‡é€‰æ¸…ç†æ–¹æ¡ˆ...")
            try:
                # åˆ é™¤å·²çŸ¥çš„çº¦æŸå’Œç´¢å¼•
                known_constraints = ["entity_name_unique"]
                for constraint in known_constraints:
                    self.graph.run(f"DROP CONSTRAINT {constraint} IF EXISTS")
                
                # åˆ é™¤å·²çŸ¥çš„ç´¢å¼•
                entity_types = ['DataStructure', 'Algorithm', 'Operation', 'Complexity', 
                              'ApplicationScenario', 'PrincipleOrProperty', 'Concept']
                known_indexes = ["entity_name_index", "entity_type_index"]
                for entity_type in entity_types:
                    known_indexes.append(f"{entity_type.lower()}_name_index")
                
                for index in known_indexes:
                    self.graph.run(f"DROP INDEX {index} IF EXISTS")
                
                # åˆ é™¤æ‰€æœ‰èŠ‚ç‚¹å’Œå…³ç³»
                self.graph.run("MATCH (n) DETACH DELETE n")
                
                # å¤‡é€‰æ–¹æ¡ˆä¹Ÿæ£€æŸ¥å±æ€§æ ‡ç­¾æ¸…ç†æƒ…å†µ
                try:
                    property_keys_result = self.graph.run("CALL db.propertyKeys()").data()
                    labels_result = self.graph.run("CALL db.labels()").data()
                    rel_types_result = self.graph.run("CALL db.relationshipTypes()").data()
                    
                    if not property_keys_result and not labels_result and not rel_types_result:
                        print("   âœ“ å±æ€§æ ‡ç­¾å·²å®Œå…¨æ¸…ç†")
                    else:
                        print(f"   âš ï¸ ä»æœ‰æ®‹ç•™ï¼šå±æ€§é”®{len(property_keys_result or [])}ä¸ªï¼Œæ ‡ç­¾{len(labels_result or [])}ä¸ªï¼Œå…³ç³»ç±»å‹{len(rel_types_result or [])}ä¸ª")
                except:
                    pass
                
                print("ğŸ§¹ å¤‡é€‰æ¸…ç†æ–¹æ¡ˆæ‰§è¡Œå®Œæˆ")
                
            except Exception as backup_e:
                print(f"å¤‡é€‰æ¸…ç†æ–¹æ¡ˆä¹Ÿå¤±è´¥: {backup_e}")
                # æœ€åçš„å…œåº•æ–¹æ¡ˆï¼šåªåˆ é™¤èŠ‚ç‚¹å’Œå…³ç³»
                try:
                    self.graph.run("MATCH (n) DETACH DELETE n")
                    print("ğŸ§¹ æœ€å°æ¸…ç†æ–¹æ¡ˆï¼šä»…åˆ é™¤èŠ‚ç‚¹å’Œå…³ç³»")
                except Exception as final_e:
                    print(f"âŒ æ‰€æœ‰æ¸…ç†æ–¹æ¡ˆéƒ½å¤±è´¥: {final_e}")
                    raise final_e

    def normalize_entity(self, entity):
        """æ ‡å‡†åŒ–å®ä½“åç§°ï¼Œæå–æ ¸å¿ƒæœ¯è¯­"""
        if not entity:
            return ""
        
        # å»é™¤å¤šä½™çš„ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
        entity = re.sub(r'\s+', ' ', entity.strip())
        
        # å»é™¤æ‹¬å·å†…å®¹ï¼ˆå¦‚ï¼šå¿«é€Ÿæ’åº(QuickSort) -> å¿«é€Ÿæ’åºï¼‰
        entity = re.sub(r'\([^)]*\)', '', entity)
        
        # å»é™¤å¼•å·
        entity = entity.replace('"', '').replace("'", '')
        
        return entity.strip()

    def get_entity_type_from_data(self, entity_name, data_type=None):
        """æ ¹æ®å®ä½“åç§°å’Œæ•°æ®ä¸­çš„ç±»å‹ä¿¡æ¯è·å–å®ä½“ç±»å‹"""
        # ä¼˜å…ˆä½¿ç”¨æ•°æ®ä¸­æä¾›çš„ç±»å‹
        if data_type and data_type in self.fallback_entity_type_dict:
            return data_type
        
        # ä»CSVåŠ è½½çš„å®ä½“ç±»å‹æ˜ å°„ä¸­æŸ¥æ‰¾
        if entity_name in self.entity_type_dict:
            return self.entity_type_dict[entity_name]
        
        # åŸºäºå®ä½“åç§°çš„å¯å‘å¼åˆ¤æ–­
        entity_lower = entity_name.lower()
        
        # ç®—æ³•ç›¸å…³
        if any(keyword in entity_lower for keyword in ['æ’åº', 'æœç´¢', 'æŸ¥æ‰¾', 'ç®—æ³•', 'sort', 'search', 'algorithm']):
            return 'Algorithm'
        
        # æ•°æ®ç»“æ„ç›¸å…³
        if any(keyword in entity_lower for keyword in ['æ ˆ', 'é˜Ÿåˆ—', 'é“¾è¡¨', 'æ ‘', 'å›¾', 'æ•°ç»„', 'å †', 'è¡¨', 'stack', 'queue', 'list', 'tree', 'graph', 'array', 'heap']):
            return 'DataStructure'
        
        # åº”ç”¨åœºæ™¯ç›¸å…³
        if any(keyword in entity_lower for keyword in ['åº”ç”¨', 'åœºæ™¯', 'æ±‚è§£', 'åŒ¹é…', 'application', 'scenario']):
            return 'ApplicationScenario'
        
        # æ“ä½œç›¸å…³
        if any(keyword in entity_lower for keyword in ['æ’å…¥', 'åˆ é™¤', 'æŸ¥æ‰¾', 'éå†', 'åˆå§‹åŒ–', 'æ‰©å®¹', 'å…¥æ ˆ', 'å‡ºæ ˆ', 'å…¥é˜Ÿ', 'å‡ºé˜Ÿ']):
            return 'Operation'
        
        # å¤æ‚åº¦ç›¸å…³
        if any(keyword in entity_lower for keyword in ['o(', 'å¤æ‚åº¦', 'complexity', 'æ—¶é—´', 'ç©ºé—´']):
            return 'Complexity'
        
        # åŸç†æˆ–å±æ€§ç›¸å…³
        if any(keyword in entity_lower for keyword in ['lifo', 'fifo', 'ç¨³å®šæ€§', 'åŸåœ°', 'æœ‰ç©·æ€§', 'ç¡®å®šæ€§', 'æœ€ä¼˜']):
            return 'PrincipleOrProperty'
        
        # é»˜è®¤è¿”å›æ¦‚å¿µç±»å‹
        return 'Concept'

    def create_nodes_with_types(self, data):
        """åˆ›å»ºæ‰€æœ‰å®ä½“èŠ‚ç‚¹ï¼Œä½¿ç”¨æ•°æ®ä¸­çš„ç±»å‹ä¿¡æ¯"""
        entities = set()
        
        # æ”¶é›†æ‰€æœ‰å®ä½“
        for item in data:
            head = self.normalize_entity(item.get('head', ''))
            tail = self.normalize_entity(item.get('tail', ''))
            
            if head:
                head_type = self.get_entity_type_from_data(head, item.get('head_type'))
                entities.add((head, head_type))
            
            if tail:
                tail_type = self.get_entity_type_from_data(tail, item.get('tail_type'))
                entities.add((tail, tail_type))
        
        # åˆ›å»ºèŠ‚ç‚¹
        created_count = 0
        for entity_name, entity_type in entities:
            if entity_name:
                # åˆ›å»ºèŠ‚ç‚¹ï¼Œä½¿ç”¨å®ä½“ç±»å‹ä½œä¸ºæ ‡ç­¾
                node = Node(entity_type, name=entity_name, type=entity_type)
                self.graph.create(node)
                created_count += 1
        
        print(f"âœ… åˆ›å»ºäº† {created_count} ä¸ªå®ä½“èŠ‚ç‚¹")
        return created_count

    def create_relationships_with_offsets(self, data):
        """åˆ›å»ºå®ä½“é—´çš„å…³ç³»ï¼ŒåŒ…å«åç§»é‡ä¿¡æ¯"""
        created_count = 0
        relation_types = set()
        
        for item in tqdm(data, desc="åˆ›å»ºå…³ç³»"):
            head = self.normalize_entity(item.get('head', ''))
            tail = self.normalize_entity(item.get('tail', ''))
            relation = item.get('relation', '')
            
            if not head or not tail or not relation:
                continue
            
            # è·å–å®ä½“ç±»å‹
            head_type = self.get_entity_type_from_data(head, item.get('head_type'))
            tail_type = self.get_entity_type_from_data(tail, item.get('tail_type'))
            
            # éªŒè¯å…³ç³»ç±»å‹çº¦æŸï¼ˆå¦‚æœæœ‰å®šä¹‰çš„è¯ï¼‰
            if relation in self.relation_dict:
                relation_info = self.relation_dict[relation]
                expected_head_type = relation_info.get('head_type')
                expected_tail_type = relation_info.get('tail_type')
                
                # å¦‚æœå®šä¹‰äº†ç±»å‹çº¦æŸï¼Œè¿›è¡ŒéªŒè¯
                if expected_head_type and head_type != expected_head_type:
                    continue
                if expected_tail_type and tail_type != expected_tail_type:
                    continue
            
            # æŸ¥æ‰¾å¤´å°¾èŠ‚ç‚¹
            head_node = self.graph.nodes.match(head_type, name=head).first()
            tail_node = self.graph.nodes.match(tail_type, name=tail).first()
            
            if head_node and tail_node:
                # åˆ›å»ºå…³ç³»ï¼Œåªä¿ç•™è¯­å¥ä¿¡æ¯
                rel_props = {
                    'source_sentence': item.get('sentence', ''),
                }
                
                relationship = Relationship(head_node, relation, tail_node, **rel_props)
                self.graph.create(relationship)
                created_count += 1
                relation_types.add(relation)
        
        print(f"âœ… åˆ›å»ºäº† {created_count} ä¸ªå…³ç³»")
        print(f"ğŸ“Š å…³ç³»ç±»å‹ç»Ÿè®¡: {dict.fromkeys(relation_types, 'âœ“')}")
        return created_count

    def create_indexes(self):
        """åˆ›å»ºç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½"""
        try:
            # ä¸ºå®ä½“åç§°åˆ›å»ºå”¯ä¸€çº¦æŸ
            self.graph.run("CREATE CONSTRAINT entity_name_unique IF NOT EXISTS FOR (n:Entity) REQUIRE n.name IS UNIQUE")
            
            # ä¸ºä¸åŒå®ä½“ç±»å‹åˆ›å»ºç´¢å¼•
            entity_types = ['DataStructure', 'Algorithm', 'Operation', 'Complexity', 'ApplicationScenario', 'PrincipleOrProperty', 'Concept']
            for entity_type in entity_types:
                self.graph.run(f"CREATE INDEX {entity_type.lower()}_name_index IF NOT EXISTS FOR (n:{entity_type}) ON (n.name)")
            
            print("âœ… ç´¢å¼•åˆ›å»ºå®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ åˆ›å»ºç´¢å¼•æ—¶å‡ºç°è­¦å‘Š: {e}")

    def load_entity_offsets_data(self, file_path=None):
        """åŠ è½½entity_offsets.jsonæ ¼å¼çš„æ•°æ®"""
        if file_path is None:
            file_path = os.path.join(self.data_path, "entity_offsets.json")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"ğŸ“ æˆåŠŸåŠ è½½æ•°æ®æ–‡ä»¶: {file_path}")
            print(f"ğŸ“Š åŸå§‹æ•°æ®æ¡æ•°: {len(data)}")
            
            # æ•°æ®é¢„å¤„ç†å’Œè¿‡æ»¤
            valid_data = []
            for item in data:
                # æ ‡å‡†åŒ–å®ä½“åç§°
                head = self.normalize_entity(item.get('head', ''))
                tail = self.normalize_entity(item.get('tail', ''))
                relation = item.get('relation', '')
                
                # è¿‡æ»¤æ— æ•ˆå…³ç³»
                if head and tail and relation and head != tail:
                    item['head'] = head
                    item['tail'] = tail
                    
                    # è®¾ç½®ç½®ä¿¡åº¦ï¼ˆå¦‚æœæ²¡æœ‰çš„è¯ï¼‰
                    if 'confidence' not in item:
                        item['confidence'] = 1.0
                    
                    # åªä¿ç•™ç½®ä¿¡åº¦é«˜äºé˜ˆå€¼çš„å…³ç³»
                    if item['confidence'] >= self.confidence:
                        valid_data.append(item)
            
            print(f"âœ… æœ‰æ•ˆæ•°æ®æ¡æ•°: {len(valid_data)}")
            return valid_data
            
        except FileNotFoundError:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return []
        except json.JSONDecodeError as e:
            print(f"âŒ JSONè§£æé”™è¯¯: {e}")
            return []
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®æ—¶å‡ºé”™: {e}")
            return []

    def load_json_data(self, file_path):
        """åŠ è½½JSONæ ¼å¼çš„æ•°æ®"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"ğŸ“ æˆåŠŸåŠ è½½æ•°æ®æ–‡ä»¶: {file_path}")
            print(f"ğŸ“Š åŸå§‹æ•°æ®æ¡æ•°: {len(data)}")
            
            # æ•°æ®é¢„å¤„ç†
            processed_data = []
            for item in data:
                if isinstance(item, dict):
                    head = self.normalize_entity(item.get('head', ''))
                    tail = self.normalize_entity(item.get('tail', ''))
                    relation = item.get('relation', item.get('predicate', ''))
                    
                    if head and tail and relation and head != tail:
                        processed_item = {
                            'head': head,
                            'tail': tail,
                            'relation': relation,
                            'confidence': item.get('confidence', 1.0),
                            'sentence': item.get('sentence', item.get('text', ''))
                        }
                        
                        if processed_item['confidence'] >= self.confidence:
                            processed_data.append(processed_item)
            
            print(f"âœ… æœ‰æ•ˆæ•°æ®æ¡æ•°: {len(processed_data)}")
            return processed_data
            
        except Exception as e:
            print(f"âŒ åŠ è½½JSONæ•°æ®æ—¶å‡ºé”™: {e}")
            return []

    def load_csv_data(self, file_path):
        """åŠ è½½CSVæ ¼å¼çš„æ•°æ®"""
        try:
            df = pd.read_csv(file_path, encoding='utf-8')
            print(f"ğŸ“ æˆåŠŸåŠ è½½æ•°æ®æ–‡ä»¶: {file_path}")
            print(f"ğŸ“Š åŸå§‹æ•°æ®æ¡æ•°: {len(df)}")
            
            processed_data = []
            for _, row in df.iterrows():
                head = self.normalize_entity(str(row.get('head', row.get('subject', ''))))
                tail = self.normalize_entity(str(row.get('tail', row.get('object', ''))))
                relation = str(row.get('relation', row.get('predicate', '')))
                
                if head and tail and relation and head != tail:
                    processed_item = {
                        'head': head,
                        'tail': tail,
                        'relation': relation,
                        'confidence': float(row.get('confidence', 1.0)),
                        'sentence': str(row.get('sentence', row.get('text', '')))
                    }
                    
                    if processed_item['confidence'] >= self.confidence:
                        processed_data.append(processed_item)
            
            print(f"âœ… æœ‰æ•ˆæ•°æ®æ¡æ•°: {len(processed_data)}")
            return processed_data
            
        except Exception as e:
            print(f"âŒ åŠ è½½CSVæ•°æ®æ—¶å‡ºé”™: {e}")
            return []

    def remove_duplicate_relationships(self, data):
        """å»é™¤é‡å¤çš„å…³ç³»"""
        seen = set()
        unique_data = []
        
        for item in data:
            # åˆ›å»ºå…³ç³»çš„å”¯ä¸€æ ‡è¯†
            key = (item['head'], item['relation'], item['tail'])
            if key not in seen:
                seen.add(key)
                unique_data.append(item)
        
        print(f"ğŸ”„ å»é‡åæ•°æ®æ¡æ•°: {len(unique_data)} (å»é™¤äº† {len(data) - len(unique_data)} æ¡é‡å¤æ•°æ®)")
        return unique_data

    def build_knowledge_graph(self, data_source='entity_offsets', file_path=None):
        """æ„å»ºçŸ¥è¯†å›¾è°±"""
        print("ğŸš€ å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±...")
        
        # æ¸…ç†æ•°æ®åº“
        self.clean_database()
        
        # æ ¹æ®æ•°æ®æºåŠ è½½æ•°æ®
        if data_source == 'entity_offsets':
            data = self.load_entity_offsets_data(file_path)
        elif data_source == 'json':
            data = self.load_json_data(file_path)
        elif data_source == 'csv':
            data = self.load_csv_data(file_path)
        else:
            print(f"âŒ ä¸æ”¯æŒçš„æ•°æ®æºç±»å‹: {data_source}")
            return
        
        if not data:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œæ„å»ºå¤±è´¥")
            return
        
        # å»é™¤é‡å¤å…³ç³»
        data = self.remove_duplicate_relationships(data)
        
        # åˆ›å»ºèŠ‚ç‚¹
        node_count = self.create_nodes_with_types(data)
        
        # åˆ›å»ºå…³ç³»
        rel_count = self.create_relationships_with_offsets(data)
        
        # åˆ›å»ºç´¢å¼•
        self.create_indexes()
        
        print(f"ğŸ‰ çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - èŠ‚ç‚¹æ•°é‡: {node_count}")
        print(f"   - å…³ç³»æ•°é‡: {rel_count}")
        print(f"   - æ•°æ®æº: {data_source}")

    def query_graph(self, query):
        """æ‰§è¡ŒCypheræŸ¥è¯¢"""
        try:
            result = self.graph.run(query)
            return result.data()
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
            return []

    def get_graph_stats(self):
        """è·å–å›¾è°±ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # èŠ‚ç‚¹ç»Ÿè®¡
            node_count = self.graph.run("MATCH (n) RETURN count(n) as count").data()[0]['count']
            
            # å…³ç³»ç»Ÿè®¡
            rel_count = self.graph.run("MATCH ()-[r]->() RETURN count(r) as count").data()[0]['count']
            
            # èŠ‚ç‚¹ç±»å‹ç»Ÿè®¡
            node_types = self.graph.run("""
                MATCH (n) 
                RETURN labels(n)[0] as type, count(n) as count 
                ORDER BY count DESC
            """).data()
            
            # å…³ç³»ç±»å‹ç»Ÿè®¡
            rel_types = self.graph.run("""
                MATCH ()-[r]->() 
                RETURN type(r) as type, count(r) as count 
                ORDER BY count DESC
            """).data()
            
            print("ğŸ“Š å›¾è°±ç»Ÿè®¡ä¿¡æ¯:")
            print(f"   - æ€»èŠ‚ç‚¹æ•°: {node_count}")
            print(f"   - æ€»å…³ç³»æ•°: {rel_count}")
            print("   - èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ:")
            for item in node_types:
                print(f"     * {item['type']}: {item['count']}")
            print("   - å…³ç³»ç±»å‹åˆ†å¸ƒ:")
            for item in rel_types:
                print(f"     * {item['type']}: {item['count']}")
                
        except Exception as e:
            print(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='æ„å»ºNeo4jçŸ¥è¯†å›¾è°±')
    parser.add_argument('--entity-offsets', type=str, nargs='?', const='default', 
                       help='ä½¿ç”¨entity_offsets.jsonæ•°æ®æºï¼ˆå¯é€‰æŒ‡å®šæ–‡ä»¶è·¯å¾„ï¼‰')
    parser.add_argument('--json', type=str, help='ä½¿ç”¨JSONæ•°æ®æºï¼ˆæŒ‡å®šæ–‡ä»¶è·¯å¾„ï¼‰')
    parser.add_argument('--csv', type=str, help='ä½¿ç”¨CSVæ•°æ®æºï¼ˆæŒ‡å®šæ–‡ä»¶è·¯å¾„ï¼‰')
    parser.add_argument('--confidence', type=float, default=0.7, help='ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.7ï¼‰')
    parser.add_argument('--stats', action='store_true', help='æ˜¾ç¤ºå›¾è°±ç»Ÿè®¡ä¿¡æ¯')
    
    args = parser.parse_args()
    
    # åˆ›å»ºçŸ¥è¯†å›¾è°±æ„å»ºå™¨
    kg_builder = Neo4jKnowledgeGraph(confidence=args.confidence)
    
    if args.stats:
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        kg_builder.get_graph_stats()
    elif args.entity_offsets:
        # ä½¿ç”¨entity_offsetsæ•°æ®æº
        file_path = None if args.entity_offsets == 'default' else args.entity_offsets
        print("ğŸ”„ ä½¿ç”¨entity_offsets.jsonæ•°æ®æº")
        kg_builder.build_knowledge_graph(data_source='entity_offsets', file_path=file_path)
    elif args.json:
        # ä½¿ç”¨JSONæ•°æ®æº
        print(f"ğŸ”„ ä½¿ç”¨JSONæ•°æ®æº: {args.json}")
        kg_builder.build_knowledge_graph(data_source='json', file_path=args.json)
    elif args.csv:
        # ä½¿ç”¨CSVæ•°æ®æº
        print(f"ğŸ”„ ä½¿ç”¨CSVæ•°æ®æº: {args.csv}")
        kg_builder.build_knowledge_graph(data_source='csv', file_path=args.csv)
    else:
        # é»˜è®¤ä½¿ç”¨entity_offsetsæ•°æ®æº
        print("ğŸ”„ ä½¿ç”¨é»˜è®¤entity_offsets.jsonæ•°æ®æº")
        kg_builder.build_knowledge_graph(data_source='entity_offsets')